{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T09:26:16.633538Z",
     "iopub.status.busy": "2025-08-14T09:26:16.633275Z",
     "iopub.status.idle": "2025-08-14T09:26:18.752384Z",
     "shell.execute_reply": "2025-08-14T09:26:18.751428Z",
     "shell.execute_reply.started": "2025-08-14T09:26:16.633519Z"
    },
    "id": "2obeKal0qrGT"
   },
   "outputs": [],
   "source": [
    "# The SageMaker Studio environment comes with most of these pre-installed.\n",
    "# This cell ensures all dependencies are present.\n",
    "!pip install -q boto3 sagemaker mlflow \"scikit-learn>=1.0\" \"pandas>=1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T15:09:41.643142Z",
     "iopub.status.busy": "2025-08-14T15:09:41.642430Z",
     "iopub.status.idle": "2025-08-14T15:09:41.647255Z",
     "shell.execute_reply": "2025-08-14T15:09:41.646318Z",
     "shell.execute_reply.started": "2025-08-14T15:09:41.643114Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Ensure MLflow is installed\n",
    "try:\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "except ImportError:\n",
    "    print(\"Installing MLflow...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",  \"boto3==1.37.1\", \"botocore==1.37.1\", \"s3transfer\", \"mlflow==2.22.0\", \"sagemaker-mlflow==0.1.0\"])\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T15:09:42.595934Z",
     "iopub.status.busy": "2025-08-14T15:09:42.595619Z",
     "iopub.status.idle": "2025-08-14T15:09:44.496552Z",
     "shell.execute_reply": "2025-08-14T15:09:44.495722Z",
     "shell.execute_reply.started": "2025-08-14T15:09:42.595891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sagemaker-mlflow\n",
      "Version: 0.1.0\n",
      "Summary: AWS Plugin for MLFlow with SageMaker\n",
      "Home-page: https://github.com/aws/sagemaker-mlflow\n",
      "Author: Amazon Web Services\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /opt/conda/lib/python3.12/site-packages\n",
      "Requires: boto3, mlflow\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show sagemaker_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T15:09:46.282458Z",
     "iopub.status.busy": "2025-08-14T15:09:46.281953Z",
     "iopub.status.idle": "2025-08-14T15:09:46.448909Z",
     "shell.execute_reply": "2025-08-14T15:09:46.448138Z",
     "shell.execute_reply.started": "2025-08-14T15:09:46.282424Z"
    },
    "id": "MlLIOYsVqrGU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created (or already exists): source\n",
      "Found MLflow Tracking Server ARN: arn:aws:sagemaker:ap-southeast-1:837028399719:mlflow-tracking-server/mlflow-ITI113-Team2\n",
      "S3 Bucket: s3://iti113-team2-bucket/Team2\n",
      "SageMaker Role ARN: arn:aws:iam::837028399719:role/iti113-team2-sagemaker-iti113-team2-domain-iti113-team2-Role\n",
      "MLflow Tracking Server ARN: arn:aws:sagemaker:ap-southeast-1:837028399719:mlflow-tracking-server/mlflow-ITI113-Team2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "import os\n",
    "\n",
    "# Setup SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# --- IMPORTANT: CONFIGURE THESE VARIABLES ---\n",
    "# s3_bucket = sagemaker_session.default_bucket()\n",
    "# ----------------------\n",
    "# UPDATE THESE VARIABLES\n",
    "bucket_name = 'iti113-team2-bucket'\n",
    "base_folder = 'Team2'\n",
    "# ----------------------\n",
    "\n",
    "# Create source folder\n",
    "folder_path = \"source\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "print(f\"Folder created (or already exists): {folder_path}\")\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "data_path = f\"s3://{bucket_name}/{base_folder}\"\n",
    "\n",
    "# Assuming you have your boto3 client and server name\n",
    "tracking_server_name = \"mlflow-ITI113-Team2\"\n",
    "\n",
    "try:\n",
    "    response = sagemaker_client.describe_mlflow_tracking_server(\n",
    "        TrackingServerName=tracking_server_name\n",
    "    )\n",
    "    tracking_server_arn = response['TrackingServerArn']\n",
    "    print(f\"Found MLflow Tracking Server ARN: {tracking_server_arn}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not find tracking server: {e}\")\n",
    "    tracking_server_arn = None\n",
    "\n",
    "# ARN of your MLflow Tracking Server\n",
    "mlflow_tracking_server_arn = tracking_server_arn\n",
    "\n",
    "# IAM role for SageMaker execution\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"S3 Bucket: {data_path}\")\n",
    "print(f\"SageMaker Role ARN: {role}\")\n",
    "print(f\"MLflow Tracking Server ARN: {mlflow_tracking_server_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Upload dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T15:09:54.909476Z",
     "iopub.status.busy": "2025-08-14T15:09:54.908275Z",
     "iopub.status.idle": "2025-08-14T15:09:54.987821Z",
     "shell.execute_reply": "2025-08-14T15:09:54.987121Z",
     "shell.execute_reply.started": "2025-08-14T15:09:54.909443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://iti113-team2-bucket/Team2/data\n"
     ]
    }
   ],
   "source": [
    "# Upload to S3\n",
    "s3_client.upload_file('Team2Dataset.csv', bucket_name, f'{base_folder}/data/Team2Dataset.csv')\n",
    "s3_path = f\"s3://{bucket_name}/{base_folder}/data/Team2Dataset.csv\"\n",
    "data_s3_uri = os.path.dirname(s3_path) # Log the directory URI\n",
    "\n",
    "print(data_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSoYELr9qrGW"
   },
   "source": [
    "-----\n",
    "\n",
    "### Creating the SageMaker Pipeline\n",
    "\n",
    "Create the pipeline scripts that will be executed as steps in our SageMaker Pipeline.\n",
    "\n",
    "#### Preprocessing Script\n",
    "\n",
    "This script will take the raw data, prepocess, split it into training and testing sets, and save them back to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T15:09:57.898563Z",
     "iopub.status.busy": "2025-08-14T15:09:57.898009Z",
     "iopub.status.idle": "2025-08-14T15:09:57.907875Z",
     "shell.execute_reply": "2025-08-14T15:09:57.907087Z",
     "shell.execute_reply.started": "2025-08-14T15:09:57.898335Z"
    },
    "id": "ScNBs_TaqrGX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/preprocess.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def preprocess(df):\n",
    "    print('preprocess-start')\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "    #drop patient identifier\n",
    "    df.drop(columns=['patientid'], inplace=True)\n",
    "    \n",
    "    df['oldpeak'] = df['oldpeak'].apply(lambda x: 0 if x < 0 else x)\n",
    "    df['oldpeak_log'] = np.log1p(df['oldpeak'])\n",
    "    df.drop(columns=['oldpeak'], inplace=True)\n",
    "    print('transform oldpeak-end')\n",
    "    \n",
    "    num_cols = ['age', 'restingBP', 'serumcholestrol', 'maxheartrate', 'oldpeak_log']\n",
    "    for col in num_cols:\n",
    "        #Replace invalid zeros with NaN \n",
    "        if col in ['restingBP', 'serumcholestrol']:  # zero Cholesterol/RestingBP is invalid\n",
    "            df[col] = df[col].replace(0, np.nan)\n",
    "        \n",
    "        #Impute NaNs with median\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        \n",
    "        #IQR-based Outlier Capping (Winsorization)\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Cap the outliers to within the IQR bounds\n",
    "        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    print('impute and remove outlier-end')\n",
    "\n",
    "    #To group categorical variable\n",
    "    # df['ChestPainType_Grouped'] = df['ChestPainType'].replace({'TA': 'Other', 'ATA': 'Other'})\n",
    "    # df = df.drop(columns=['ChestPainType'])\n",
    "    # df = pd.get_dummies(df, columns=['ChestPainType_Grouped'], drop_first=False)\n",
    "    \n",
    "    #One-Hot Encoding on categorical variable\n",
    "    df = pd.get_dummies(df, columns=['chestpain'], drop_first=False) #Keep all dummy columns\n",
    "    df = pd.get_dummies(df, columns=['restingrelectro'], drop_first=False)\n",
    "    df = pd.get_dummies(df, columns=['slope'], drop_first=False)\n",
    "    df = pd.get_dummies(df, columns=['noofmajorvessels'], drop_first=False) \n",
    "    print('encoding-end')\n",
    "    \n",
    "    print(f\"Dataset: {df.head(2)}\")\n",
    "    print('preprocess-end')\n",
    "\n",
    "    return df\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-path\", type=str, help=\"Directory containing Team2Dataset.csv\")\n",
    "    parser.add_argument(\"--output-train-path\", type=str, help=\"Output directory for train.csv\")\n",
    "    parser.add_argument(\"--output-test-path\", type=str, help=\"Output directory for test.csv\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Use provided paths or fall back to SageMaker defaults\n",
    "    input_path = args.input_path or \"/opt/ml/processing/input\"\n",
    "    output_train_path = args.output_train_path or \"/opt/ml/processing/train\"\n",
    "    output_test_path = args.output_test_path or \"/opt/ml/processing/test\"\n",
    "\n",
    "    input_file = os.path.join(input_path, \"Team2Dataset.csv\")\n",
    "    print(f\"Reading input file from {input_file}...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    df = preprocess(df) #clean data\n",
    "    \n",
    "    print(\"Splitting into train/test...\")\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    os.makedirs(output_train_path, exist_ok=True)\n",
    "    os.makedirs(output_test_path, exist_ok=True)\n",
    "\n",
    "    train_output = os.path.join(output_train_path, \"train.csv\")\n",
    "    test_output = os.path.join(output_test_path, \"test.csv\")\n",
    "\n",
    "    print(f\"Saving train to {train_output}\")\n",
    "    train.to_csv(train_output, index=False)\n",
    "\n",
    "    print(f\"Saving test to {test_output}\")\n",
    "    test.to_csv(test_output, index=False)\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjXii-anqrGY"
   },
   "source": [
    "#### Training Script\n",
    "\n",
    "This script will train a model on the preprocessed data and log the results to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T15:18:54.264662Z",
     "iopub.status.busy": "2025-08-14T15:18:54.264374Z",
     "iopub.status.idle": "2025-08-14T15:18:54.269897Z",
     "shell.execute_reply": "2025-08-14T15:18:54.269078Z",
     "shell.execute_reply.started": "2025-08-14T15:18:54.264640Z"
    },
    "id": "nDua4mVRqrGY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/train.py\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# # Ensure MLflow is installed\n",
    "try:\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "except ImportError:\n",
    "    print(\"Installing MLflow...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",  \"boto3==1.37.1\", \"botocore==1.37.1\", \"s3transfer\", \"mlflow==2.22.0\", \"sagemaker-mlflow==0.1.0\"])\n",
    "    import mlflow\n",
    "    import sagemaker_mlflow\n",
    "    \n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--tracking_server_arn\", type=str, required=True)\n",
    "parser.add_argument(\"--experiment_name\", type=str, default=\"Default\")\n",
    "parser.add_argument(\"--model_output_path\", type=str, default=\"/opt/ml/model\")\n",
    "parser.add_argument(\"-C\", \"--C\", type=float, default=0.5)\n",
    "parser.add_argument(\"--run_name\", type=str, default=\"Experiment-LR\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print('Start-Train')\n",
    "# Load training data\n",
    "train_path = glob.glob(\"/opt/ml/input/data/train/*.csv\")[0]\n",
    "df = pd.read_csv(train_path)\n",
    "print(df.head())\n",
    "\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "# # Set up MLflow\n",
    "mlflow.set_tracking_uri(args.tracking_server_arn)\n",
    "mlflow.set_experiment(args.experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=args.run_name) as run:\n",
    "    mlflow.log_param(\"C\", args.C)\n",
    "    model = LogisticRegression(C=args.C)\n",
    "    model.fit(X, y)\n",
    "    acc = accuracy_score(y, model.predict(X))\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    mlflow.sklearn.log_model(sk_model=model, artifact_path=\"model\")\n",
    "\n",
    "    os.makedirs(args.model_output_path, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(args.model_output_path, \"model.joblib\"))\n",
    "    with open(os.path.join(args.model_output_path, \"run_id.txt\"), \"w\") as f:\n",
    "        f.write(run.info.run_id)\n",
    "\n",
    "    print(f\"Training complete. Accuracy: {acc:.4f}\")\n",
    "    print(f\"MLflow Run ID: {run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNSpMrDsqrGY"
   },
   "source": [
    "#### Evaluation Script\n",
    "\n",
    "This script evaluates the model and creates an evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T15:10:00.799413Z",
     "iopub.status.busy": "2025-08-14T15:10:00.799126Z",
     "iopub.status.idle": "2025-08-14T15:10:00.805447Z",
     "shell.execute_reply": "2025-08-14T15:10:00.804742Z",
     "shell.execute_reply.started": "2025-08-14T15:10:00.799391Z"
    },
    "id": "s8Ed1lCaqrGZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/evaluate.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import tarfile\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Parse Arguments ---\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model-path\", type=str, required=True, help=\"Path to the directory containing the model.tar.gz file.\")\n",
    "    parser.add_argument(\"--test-path\", type=str, required=True, help=\"Path to the directory containing test.csv.\")\n",
    "    parser.add_argument(\"--output-path\", type=str, required=True, help=\"Path to save the evaluation.json report.\")\n",
    "    parser.add_argument(\"--model-package-group-name\", type=str, required=True, help=\"Name of the SageMaker Model Package Group.\")\n",
    "    parser.add_argument(\"--region\", type=str, required=True, help=\"The AWS region for creating the boto3 client.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # --- Extract and Load Model ---\n",
    "    # SageMaker packages models in a .tar.gz file. We need to extract it first.\n",
    "    model_archive_path = os.path.join(args.model_path, 'model.tar.gz')\n",
    "    print(f\"Extracting model from archive: {model_archive_path}\")\n",
    "    with tarfile.open(model_archive_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=args.model_path)\n",
    "\n",
    "    # Load the model using joblib\n",
    "    model_file_path = os.path.join(args.model_path, \"model.joblib\")\n",
    "    if not os.path.exists(model_file_path):\n",
    "        raise FileNotFoundError(f\"Model file 'model.joblib' not found after extraction in: {args.model_path}\")\n",
    "    \n",
    "    print(f\"Loading model from: {model_file_path}\")\n",
    "    model = joblib.load(model_file_path)\n",
    "\n",
    "    # --- Prepare Data and Evaluate ---\n",
    "    test_file_path = os.path.join(args.test_path, \"test.csv\")\n",
    "    if not os.path.exists(test_file_path):\n",
    "        raise FileNotFoundError(f\"Test data not found: {test_file_path}\")\n",
    "    \n",
    "    test_df = pd.read_csv(test_file_path)\n",
    "    X_test = test_df.drop(\"target\", axis=1)\n",
    "    y_test = test_df[\"target\"]\n",
    "    \n",
    "    print(\"Running predictions on the test dataset.\")\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    report = {\"accuracy\": accuracy}\n",
    "    print(f\"Calculated accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # --- Check for Existing Baseline Model in SageMaker Model Registry ---\n",
    "    print(f\"Checking for baseline model in region: {args.region}\")\n",
    "    sagemaker_client = boto3.client(\"sagemaker\", region_name=args.region)\n",
    "    try:\n",
    "        response = sagemaker_client.list_model_packages(\n",
    "            ModelPackageGroupName=args.model_package_group_name,\n",
    "            ModelApprovalStatus=\"Approved\",\n",
    "            SortBy=\"CreationTime\",\n",
    "            SortOrder=\"Descending\",\n",
    "            MaxResults=1,\n",
    "        )\n",
    "        # If the list is not empty, an approved model already exists\n",
    "        report[\"baseline_exists\"] = len(response[\"ModelPackageSummaryList\"]) > 0\n",
    "        if report[\"baseline_exists\"]:\n",
    "            print(f\"An approved baseline model was found in '{args.model_package_group_name}'.\")\n",
    "        else:\n",
    "             print(f\"No approved baseline model was found in '{args.model_package_group_name}'.\")\n",
    "\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        # If the ModelPackageGroup doesn't exist, there is no baseline\n",
    "        if \"ResourceNotFound\" in str(e):\n",
    "            report[\"baseline_exists\"] = False\n",
    "            print(f\"Model Package Group '{args.model_package_group_name}' not found. Assuming no baseline exists.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # --- Write Final Report ---\n",
    "    os.makedirs(args.output_path, exist_ok=True)\n",
    "    report_path = os.path.join(args.output_path, \"evaluation.json\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "        \n",
    "    print(f\"Evaluation complete. Report written to: {report_path}\")\n",
    "    print(\"Evaluation Report:\")\n",
    "    print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQFqaaTJqrGZ"
   },
   "source": [
    "### Pipeline Definition\n",
    "\n",
    "Define the SageMaker Pipeline using the scripts we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T15:19:21.678783Z",
     "iopub.status.busy": "2025-08-14T15:19:21.678379Z",
     "iopub.status.idle": "2025-08-14T15:19:24.026879Z",
     "shell.execute_reply": "2025-08-14T15:19:24.025882Z",
     "shell.execute_reply.started": "2025-08-14T15:19:21.678755Z"
    },
    "id": "wJVkKxWbqrGZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team2PredictionPipeline is defined and ready to be executed.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, TrainingInput\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.conditions import ConditionNot\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.parameters import ParameterFloat, ParameterString\n",
    "from sagemaker.model_metrics import ModelMetrics, FileSource\n",
    "\n",
    "# Parameters\n",
    "model_package_group_name = \"Team2PredictorModels\"\n",
    "processing_instance_type = \"ml.t3.medium\"\n",
    "training_instance_type = \"ml.m5.large\"\n",
    "experiment_name_param = ParameterString(name=\"ExperimentName\", default_value=\"Team2-Prediction\")\n",
    "accuracy_threshold_param = ParameterFloat(name=\"AccuracyThreshold\", default_value=0.80)\n",
    "model_c_param = ParameterFloat(name=\"C\", default_value=0.5)\n",
    "run_name_param = ParameterString(name=\"RunName\", default_value=\"Experiment-LR\")\n",
    "\n",
    "#set processing test folder dest\n",
    "s3_process_train_path = f\"s3://{bucket_name}/{base_folder}/processing/train\"\n",
    "s3_process_test_path = f\"s3://{bucket_name}/{base_folder}/processing/test\"\n",
    "# print(s3_process_train_path)\n",
    "# print(s3_process_test_path)\n",
    "\n",
    "preprocessor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", sagemaker_session.boto_region_name, \"1.2-1\"),\n",
    "    command=[\n",
    "        \"python3\",\n",
    "    ],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "step_preprocess = ProcessingStep(\n",
    "    name=\"PreprocessData\",\n",
    "    processor=preprocessor,\n",
    "    inputs=[ProcessingInput(source=data_s3_uri, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination=s3_process_train_path),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination=s3_process_test_path),\n",
    "    ],\n",
    "    code=\"source/preprocess.py\",\n",
    ")\n",
    "\n",
    "# Training Step\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"source\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=training_instance_type,\n",
    "    role=role,\n",
    "    hyperparameters={\n",
    "        \"tracking_server_arn\": mlflow_tracking_server_arn,\n",
    "        \"experiment_name\": experiment_name_param,\n",
    "        \"C\": model_c_param,\n",
    "        \"model_output_path\": \"/opt/ml/model\",\n",
    "        \"run_name\": run_name_param,\n",
    "    },\n",
    "    py_version=\"py3\",\n",
    "    requirements=\"source/requirements.txt\"\n",
    ")\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainModel\",\n",
    "    estimator=sklearn_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_preprocess.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# Evaluation Step\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", sagemaker_session.boto_region_name, \"1.2-1\"),\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"evaluate-model\",\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"EvaluateModel\",\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_preprocess.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")],\n",
    "    code=\"source/evaluate.py\",  # SageMaker will handle uploading and running this script\n",
    "    job_arguments=[  # Pass arguments here instead of in command\n",
    "        \"--model-path\", \"/opt/ml/processing/model\",\n",
    "        \"--test-path\", \"/opt/ml/processing/test\",\n",
    "        \"--output-path\", \"/opt/ml/processing/evaluation\",\n",
    "        \"--model-package-group-name\", model_package_group_name,\n",
    "        \"--region\", \"ap-southeast-1\",\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "\n",
    "model_metrics_report = ModelMetrics(\n",
    "    model_statistics=FileSource(\n",
    "        s3_uri=step_eval.properties.ProcessingOutputConfig.Outputs[\"evaluation\"].S3Output.S3Uri,\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# RegisterModel step (always defined, but executed conditionally)\n",
    "step_register_new = RegisterModel(\n",
    "    name=\"RegisterNewModel\",\n",
    "    estimator=sklearn_estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"Team2PredictorModels\",\n",
    "    model_metrics=model_metrics_report,\n",
    "    approval_status=\"PendingManualApproval\",\n",
    ")\n",
    "\n",
    "step_register_better_model = RegisterModel(\n",
    "    name=\"RegisterBetterModel\",\n",
    "    estimator=sklearn_estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"Team2PredictorModels\",\n",
    "    model_metrics=model_metrics_report,\n",
    "    approval_status=\"PendingManualApproval\",\n",
    ")\n",
    "\n",
    "\n",
    "# Conditions: check accuracy > threshold OR no model exists\n",
    "cond_accuracy = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"accuracy\"\n",
    "    ),\n",
    "    right=accuracy_threshold_param\n",
    ")\n",
    "\n",
    "cond_no_registered = ConditionEquals(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"baseline_exists\" # Check the key added to the report\n",
    "    ),\n",
    "    right=False # Condition is TRUE if baseline_exists is False\n",
    ")\n",
    "\n",
    "# Outer step: Check if model is better\n",
    "step_cond_accuracy = ConditionStep(\n",
    "    name=\"CheckAccuracy\",\n",
    "    conditions=[cond_accuracy],\n",
    "    if_steps=[step_register_better_model], # Register model if accuracy is high\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "# Checks for existence of registered model first\n",
    "step_cond_no_registered = ConditionStep(\n",
    "    name=\"CheckIfModelExists\",\n",
    "    conditions=[cond_no_registered],\n",
    "    if_steps=[step_register_new],  # Register model if no baseline exists\n",
    "    else_steps=[step_cond_accuracy],\n",
    ")\n",
    "\n",
    "\n",
    "# Define Pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"Team2PredictionPipeline\",\n",
    "    parameters=[experiment_name_param, accuracy_threshold_param, model_c_param, run_name_param],\n",
    "    steps=[step_preprocess, step_train, step_eval, step_cond_no_registered] # Use the 'no registered model' check as the primary condition step\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "print(\"Team2PredictionPipeline is defined and ready to be executed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T15:19:55.193624Z",
     "iopub.status.busy": "2025-08-14T15:19:55.192905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline execution.\n",
      "arn:aws:sagemaker:ap-southeast-1:837028399719:pipeline/Team2PredictionPipeline/execution/24pbe7v9t9ha\n"
     ]
    }
   ],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"C\": 0.5,\n",
    "        \"RunName\": \"Experiment-v1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Pipeline execution.\")\n",
    "print(execution.arn)\n",
    "execution.wait()\n",
    "execution.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team2PredictionPipeline - Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both model from the 'C-1.0' and 'C-0.5' archived accuracy of 0.8555.\n",
    "The model from the 'C-0.5' run was selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqKCtZZcqrGa",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create Deployment Pipeline\n",
    "\n",
    "Create a separate pipeline that is triggered by a new model registration. This pipeline will deploy the model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:39:13.301537Z",
     "iopub.status.busy": "2025-07-28T08:39:13.300894Z",
     "iopub.status.idle": "2025-07-28T08:39:13.307132Z",
     "shell.execute_reply": "2025-07-28T08:39:13.306385Z",
     "shell.execute_reply.started": "2025-07-28T08:39:13.301492Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile source/inference.py\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "\n",
    "    print(\"Loading model from a .joblib file.\")\n",
    "    # The model is saved as 'model.joblib' in your training script.\n",
    "    model_path = os.path.join(model_dir, \"model.joblib\")\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "        print(\"Model loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "\n",
    "    print(f\"Received request of type: {request_content_type}\")\n",
    "    if request_content_type == 'application/json':\n",
    "        try:\n",
    "            # Assuming the JSON input is in the format: {\"data\": [[...], [...]]}\n",
    "            data = json.loads(request_body)\n",
    "            \n",
    "            if \"data\" not in data or not isinstance(data[\"data\"], list):\n",
    "                raise ValueError(\"JSON must contain a 'data' field with a list of row dictionaries.\")\n",
    "\n",
    "            df = pd.DataFrame(data['data'])\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing JSON: {e}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "\n",
    "    print(\"Making predictions on the input data.\")\n",
    "    try:\n",
    "        predictions = model.predict(input_data)\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error during prediction: {e}\")\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    print(f\"Serializing prediction to: {response_content_type}\")\n",
    "    if response_content_type == 'application/json':\n",
    "        try:\n",
    "            # Convert numpy array to a list and wrap it in a JSON object.\n",
    "            response = {\"predictions\": prediction.tolist()}\n",
    "            return json.dumps(response)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error serializing prediction to JSON: {e}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported response content type: {response_content_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment Script\n",
    "\n",
    "This script will take the registered model and deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:39:21.366145Z",
     "iopub.status.busy": "2025-07-28T08:39:21.365893Z",
     "iopub.status.idle": "2025-07-28T08:39:21.371483Z",
     "shell.execute_reply": "2025-07-28T08:39:21.370909Z",
     "shell.execute_reply.started": "2025-07-28T08:39:21.366126Z"
    },
    "id": "1objt2YBqrGa"
   },
   "outputs": [],
   "source": [
    "%%writefile source/deploy.py\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# --- Install required packages ---\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"boto3==1.28.57\", \"botocore==1.31.57\", \"numpy==1.24.1\", \"sagemaker\" ])\n",
    "\n",
    "# Ensure sagemaker SDK is installed before importing\n",
    "try:\n",
    "    import sagemaker\n",
    "except ImportError:\n",
    "    print(\"sagemaker SDK not found. Installing now...\")\n",
    "    install(\"sagemaker\")\n",
    "    import sagemaker\n",
    "\n",
    "import argparse\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.model import Model\n",
    "import shutil\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Accept the registered model's ARN instead of the S3 data path\n",
    "    parser.add_argument(\"--model-package-arn\", type=str, required=True)\n",
    "    parser.add_argument(\"--role\", type=str, required=True)\n",
    "    parser.add_argument(\"--endpoint-name\", type=str, required=True)\n",
    "    parser.add_argument(\"--region\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    boto_session = boto3.Session(region_name=args.region)\n",
    "    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "    sm_client = boto3.client(\"sagemaker\", region_name=args.region)\n",
    "\n",
    "    # --- Step 1: Get Model Artifacts from the Model Package ---\n",
    "    print(f\"Describing model package: {args.model_package_arn}\")\n",
    "    model_package_description = sm_client.describe_model_package(ModelPackageName=args.model_package_arn)\n",
    "    \n",
    "    # Extract the S3 path to the model artifacts (model.tar.gz)\n",
    "    model_artifacts = model_package_description[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]\n",
    "\n",
    "    # Extract the container image URI\n",
    "    image_uri = model_package_description[\"InferenceSpecification\"][\"Containers\"][0][\"Image\"]\n",
    "\n",
    "    print(f\"Found model artifacts at: {model_artifacts}\")\n",
    "    print(f\"Using container image: {image_uri}\")\n",
    "\n",
    "    # --- Step 2: Prepare a clean directory for the inference code ---\n",
    "    original_code_location = \"/opt/ml/processing/input/scripts\"\n",
    "    inference_script_path = os.path.join(original_code_location, \"inference.py\")\n",
    "    clean_code_dir = \"/tmp/code\"\n",
    "\n",
    "    if not os.path.exists(inference_script_path):\n",
    "        raise FileNotFoundError(f\"inference.py not found at {inference_script_path}. Did you include it via ProcessingInput?\")\n",
    "    \n",
    "    # Create the clean directory, removing it first if it exists\n",
    "    if os.path.exists(clean_code_dir):\n",
    "        shutil.rmtree(clean_code_dir)\n",
    "    os.makedirs(clean_code_dir)\n",
    "\n",
    "    # Copy only the inference script to the clean directory\n",
    "    shutil.copy(inference_script_path, clean_code_dir)\n",
    "    print(f\"Copied inference.py to clean dir: {clean_code_dir}\")\n",
    "    \n",
    "    # --- Step 2: Create a SageMaker Model object using the local inference.py ---\n",
    "    # This explicitly tells SageMaker to use your provided inference script.\n",
    "    model = Model(\n",
    "        image_uri=image_uri,\n",
    "        model_data=model_artifacts, # Use artifacts from the registered model\n",
    "        role=args.role,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        entry_point=\"inference.py\",  # Explicitly use your inference script\n",
    "        source_dir=clean_code_dir         # Directory containing inference.py\n",
    "    )\n",
    "   \n",
    "    # First, try to delete existing resources to ensure a clean deployment\n",
    "    try:\n",
    "        # Delete the endpoint first\n",
    "        sm_client.delete_endpoint(EndpointName=args.endpoint_name)\n",
    "        print(f\"Deleted existing endpoint: {args.endpoint_name}\")\n",
    "        \n",
    "        # Then, delete the endpoint config\n",
    "        sm_client.delete_endpoint_config(EndpointConfigName=args.endpoint_name)\n",
    "        print(f\"Deleted existing endpoint config: {args.endpoint_name}\")\n",
    "    except sm_client.exceptions.ClientError as e:\n",
    "        # If the resources don't exist, that's fine.\n",
    "        if \"Could not find\" not in str(e):\n",
    "            raise e\n",
    "    \n",
    "    # Deploy the model to an endpoint\n",
    "    print(f\"Deploying registered model from ARN to endpoint: {args.endpoint_name}\")\n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.t2.medium\",\n",
    "        endpoint_name=args.endpoint_name,\n",
    "        # Update endpoint if it already exists\n",
    "        update_endpoint=True\n",
    "    )\n",
    "    print(\"Deployment complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQppkx_5qrGa"
   },
   "source": [
    "#### Deployment Pipeline Definition\n",
    "\n",
    "This pipeline will be triggered when a new model is registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:39:29.748254Z",
     "iopub.status.busy": "2025-07-28T08:39:29.747939Z",
     "iopub.status.idle": "2025-07-28T08:39:32.898326Z",
     "shell.execute_reply": "2025-07-28T08:39:32.897722Z",
     "shell.execute_reply.started": "2025-07-28T08:39:29.748231Z"
    },
    "id": "4E3VbMYxqrGb"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "import sagemaker\n",
    "\n",
    "# Define Parameters for the deployment pipeline\n",
    "# This will be provided by the EventBridge trigger\n",
    "model_package_arn_param = ParameterString(name=\"ModelPackageArn\", default_value=\"\")\n",
    "role_param = ParameterString(name=\"ExecutionRole\", default_value=role)\n",
    "endpoint_name_param = ParameterString(name=\"EndpointName\", default_value=\"heartdisease-predictor-endpoint\")\n",
    "\n",
    "# Create a ScriptProcessor for deployment\n",
    "# Using a more recent scikit-learn version is generally a good idea\n",
    "deploy_processor = ScriptProcessor(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"sklearn\", sagemaker_session.boto_region_name, version=\"1.2-1\"),\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=role_param,\n",
    "    base_job_name=\"deploy-registered-model\"\n",
    ")\n",
    "\n",
    "# Define the deployment step that takes the model ARN as an argument\n",
    "step_deploy = ProcessingStep(\n",
    "    name=\"DeployRegisteredModel\",\n",
    "    processor=deploy_processor,\n",
    "    inputs=[ProcessingInput(source=\"source/\", destination=\"/opt/ml/processing/input/scripts\")],\n",
    "    code=\"source/deploy.py\",\n",
    "    job_arguments=[\n",
    "        \"--model-package-arn\", model_package_arn_param,\n",
    "        \"--role\", role_param,\n",
    "        \"--endpoint-name\", endpoint_name_param,\n",
    "        \"--region\", \"ap-southeast-1\" \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the independent deployment pipeline\n",
    "deploy_pipeline = Pipeline(\n",
    "    name=\"HeartDiseaseDeployPipeline\",\n",
    "    parameters=[model_package_arn_param, role_param, endpoint_name_param],\n",
    "    steps=[step_deploy],\n",
    ")\n",
    "\n",
    "# Create or update the pipeline definition\n",
    "# Capture the response which contains the ARN\n",
    "response = deploy_pipeline.upsert(role_arn=role)\n",
    "\n",
    "# Extract the ARN from the response dictionary\n",
    "pipeline_arn = response['PipelineArn']\n",
    "\n",
    "print(f\"Deployment pipeline ARN: {pipeline_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Deploy endpoint manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:41:31.462963Z",
     "iopub.status.busy": "2025-07-28T08:41:31.462630Z",
     "iopub.status.idle": "2025-07-28T08:49:33.404393Z",
     "shell.execute_reply": "2025-07-28T08:49:33.403789Z",
     "shell.execute_reply.started": "2025-07-28T08:41:31.462939Z"
    }
   },
   "outputs": [],
   "source": [
    "response = sagemaker_client.list_model_packages(ModelPackageGroupName=\"HeartDiseasePredictorModels\")\n",
    "\n",
    "#approve PendingManualApproval model\n",
    "for model in response[\"ModelPackageSummaryList\"]:\n",
    "    if model[\"ModelApprovalStatus\"] == \"PendingManualApproval\":\n",
    "        print(\"Approving:\", model[\"ModelPackageArn\"])\n",
    "        sagemaker_client.update_model_package(\n",
    "            ModelPackageArn=model[\"ModelPackageArn\"],\n",
    "            ModelApprovalStatus=\"Approved\",\n",
    "            ApprovalDescription=\"Auto-approved after validation.\"\n",
    "        )\n",
    "\n",
    "response = sagemaker_client.list_model_packages(ModelPackageGroupName=\"HeartDiseasePredictorModels\")\n",
    "#get recent approved model\n",
    "for model in response['ModelPackageSummaryList']:\n",
    "    if model['ModelApprovalStatus'] == 'Approved':\n",
    "        model_package_arn = model['ModelPackageArn']\n",
    "        print(\"Approved ModelPackageArn:\", model_package_arn)\n",
    "        break\n",
    "\n",
    "execution = deploy_pipeline.start(\n",
    "    parameters={\n",
    "        \"ModelPackageArn\": model_package_arn\n",
    "    }\n",
    ")\n",
    "print(\"Pipeline execution.\")\n",
    "print(execution.arn)\n",
    "execution.wait()\n",
    "execution.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeployPipeline Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Invoking the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T08:53:47.403425Z",
     "iopub.status.busy": "2025-07-28T08:53:47.402900Z",
     "iopub.status.idle": "2025-07-28T08:53:48.602283Z",
     "shell.execute_reply": "2025-07-28T08:53:48.601694Z",
     "shell.execute_reply.started": "2025-07-28T08:53:47.403399Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "# Define endpoint name using the same name deployed\n",
    "endpoint_name = \"heartdisease-predictor-endpoint\"\n",
    "\n",
    "aws_region = \"ap-southeast-1\"\n",
    "\n",
    "# Create a client to interact with the SageMaker endpoint\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\", region_name=aws_region)\n",
    "\n",
    "# 2. Prepare your test data (payload)\n",
    "s3 = boto3.client(\"s3\")\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = 'iti113-team2-bucket'\n",
    "base_folder = 'Team2'\n",
    "\n",
    "#test with test.csv\n",
    "s3_process_test_path = f\"s3://{bucket_name}/{base_folder}/processing/test/test.csv\"\n",
    "df = pd.read_csv(s3_process_test_path)\n",
    "df = df.drop(\"HeartDisease\", axis=1)\n",
    "\n",
    "#send test records\n",
    "sample_data_point = df.head(5).to_dict(orient=\"records\")\n",
    "payload = {\"data\": sample_data_point}\n",
    "print(f\"Sending payload: {json.dumps(payload)}\")\n",
    "\n",
    "# 3. Invoke the endpoint and get the prediction\n",
    "try:\n",
    "    response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_arn = response[\"EndpointArn\"]\n",
    "    print(\"\\nEndpoint ARN:\", endpoint_arn)\n",
    "\n",
    "    response = sagemaker_runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload) # Serialize the payload to a JSON string\n",
    "    )\n",
    "\n",
    "    # The response body is a streaming object, so we need to read and decode it\n",
    "    response_body = response['Body'].read()\n",
    "    result = json.loads(response_body.decode('utf-8'))\n",
    "\n",
    "    print(\"\\nSuccess!\")\n",
    "    print(f\"Prediction result: {result}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error invoking endpoint: {e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### View Cloudwatch Logs\n",
    "\n",
    "You can view the cloudwatch logs. Here is an example for the logs of a previous endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T06:19:04.922948Z",
     "iopub.status.busy": "2025-07-28T06:19:04.922393Z",
     "iopub.status.idle": "2025-07-28T06:19:24.707008Z",
     "shell.execute_reply": "2025-07-28T06:19:24.706169Z",
     "shell.execute_reply.started": "2025-07-28T06:19:04.922921Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Enter the name of your SageMaker endpoint\n",
    "endpoint_name = \"heartdisease-predictor-endpoint\"\n",
    "\n",
    "# The log group is created based on the endpoint name\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "\n",
    "# Create a CloudWatch Logs client\n",
    "logs_client = boto3.client(\"logs\")\n",
    "\n",
    "print(f\"Searching for logs in: {log_group_name}\\n\")\n",
    "\n",
    "try:\n",
    "    # Find all log streams in the log group, ordered by the most recent\n",
    "    response = logs_client.describe_log_streams(\n",
    "        logGroupName=log_group_name,\n",
    "        orderBy='LastEventTime',\n",
    "        descending=True\n",
    "    )\n",
    "\n",
    "    log_streams = response.get(\"logStreams\", [])\n",
    "\n",
    "    if not log_streams:\n",
    "        print(\"No log streams found. The endpoint might not have processed any requests yet.\")\n",
    "    \n",
    "    # Loop through each stream and print its recent log events\n",
    "    for stream in log_streams:\n",
    "        stream_name = stream['logStreamName']\n",
    "        print(f\"--- Logs from stream: {stream_name} ---\")\n",
    "\n",
    "        # Get log events from the stream\n",
    "        log_events = logs_client.get_log_events(\n",
    "            logGroupName=log_group_name,\n",
    "            logStreamName=stream_name,\n",
    "            startFromHead=False,  # False gets recent logs first\n",
    "            limit=50  # Get up to 50 recent log events\n",
    "        )\n",
    "        \n",
    "        # Print events in chronological order\n",
    "        for event in reversed(log_events.get(\"events\", [])):\n",
    "            print(event['message'].strip())\n",
    "        \n",
    "        print(\"-\" * (len(stream_name) + 24), \"\\n\")\n",
    "\n",
    "except logs_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"Error: Log group '{log_group_name}' was not found.\")\n",
    "    print(\"Please check the endpoint name and ensure it has been invoked.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
