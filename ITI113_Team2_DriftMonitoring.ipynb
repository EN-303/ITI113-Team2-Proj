{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e9b31d1-6843-47ad-b2bb-fb91f27e4578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:19:48.068326Z",
     "iopub.status.busy": "2025-08-25T15:19:48.067915Z",
     "iopub.status.idle": "2025-08-25T15:19:48.432074Z",
     "shell.execute_reply": "2025-08-25T15:19:48.431343Z",
     "shell.execute_reply.started": "2025-08-25T15:19:48.068302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::837028399719:role/iti113-team2-sagemaker-iti113-team2-domain-iti113-team2-Role\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "# Setup SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014517f-019a-47c6-b421-bc863048ffd6",
   "metadata": {},
   "source": [
    "----\n",
    "#### Generate the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e105b08-6d8d-4eab-b212-9b4c1fa2b4f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T14:21:55.181820Z",
     "iopub.status.busy": "2025-08-25T14:21:55.181452Z",
     "iopub.status.idle": "2025-08-25T14:27:18.050529Z",
     "shell.execute_reply": "2025-08-25T14:27:18.049729Z",
     "shell.execute_reply.started": "2025-08-25T14:21:55.181797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-08-25-14-21-55-214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................\u001b[34m2025-08-25 14:24:41.081820: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:41.081871: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:43.044505: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:43.044545: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:43.044574: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-184-154.ap-southeast-1.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:43.044909: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,147 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:ap-southeast-1:837028399719:processing-job/baseline-suggestion-job-2025-08-25-14-21-55-214', 'ProcessingJobName': 'baseline-suggestion-job-2025-08-25-14-21-55-214', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '245545462676.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://iti113-team2-bucket/Team2/processing/train/v1/train.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://iti113-team2-bucket/Team2/monitoring/baseline', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.t3.large', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::837028399719:role/iti113-team2-sagemaker-iti113-team2-domain-iti113-team2-Role', 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,147 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,147 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,148 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,148 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,148 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,486 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,487 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,487 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.t3.large', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.t3.large', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,501 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,501 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:45,501 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:47,393 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.184.154\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar\u001b[0m\n",
      "\u001b[34m:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:47,412 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:47,420 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-9fa65061-5f5e-4b4d-a1af-cdb0ecc4ea61\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,474 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,499 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,500 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,505 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,528 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,528 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,528 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,529 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,586 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,611 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,611 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,616 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,620 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Aug 25 14:24:48\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,622 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,622 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,625 INFO util.GSet: 2.0% max memory 1.4 GB = 28.9 MB\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,626 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,684 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,689 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,689 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,689 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,689 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,689 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,689 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,689 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,690 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,690 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,690 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,690 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,739 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,739 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,739 INFO util.GSet: 1.0% max memory 1.4 GB = 14.5 MB\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,739 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,741 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,741 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,741 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,741 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,747 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,753 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,753 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,753 INFO util.GSet: 0.25% max memory 1.4 GB = 3.6 MB\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,753 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,763 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,763 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,763 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,766 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,766 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,769 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,769 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,770 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 444.7 KB\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,770 INFO util.GSet: capacity      = 2^16 = 65536 entries\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,820 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1799540285-10.0.184.154-1756131888810\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,839 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,858 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,975 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:48,996 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:49,004 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.184.154\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:49,016 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:51,093 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:51,094 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:53,207 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:53,207 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:55,595 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:55,595 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:58,050 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:24:58,051 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:00,653 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:00,654 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:10,664 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:13,789 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:14,342 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:14,391 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:14,418 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,710 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,745 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,746 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,746 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,748 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,818 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 5744, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,833 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,835 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,948 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,949 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,949 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,949 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:15,950 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:16,549 INFO util.Utils: Successfully started service 'sparkDriver' on port 41763.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:16,609 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:16,677 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:16,716 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:16,717 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:16,779 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:16,846 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-2f796eca-0fc0-4a75-a35e-b569b55e6c87\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:16,875 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:16,951 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:17,014 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.184.154:41763/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1756131915703\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:18,090 INFO client.RMProxy: Connecting to ResourceManager at /10.0.184.154:8032\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:19,051 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:19,052 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:19,065 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (7834 MB per container)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:19,066 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:19,066 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:19,067 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:19,076 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:19,194 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:21,996 INFO yarn.Client: Uploading resource file:/tmp/spark-95431795-e976-4fd8-a1ef-4a1c568c83b7/__spark_libs__5349549257799812426.zip -> hdfs://10.0.184.154/user/root/.sparkStaging/application_1756131897932_0001/__spark_libs__5349549257799812426.zip\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:24,360 INFO yarn.Client: Uploading resource file:/tmp/spark-95431795-e976-4fd8-a1ef-4a1c568c83b7/__spark_conf__7718051216649129729.zip -> hdfs://10.0.184.154/user/root/.sparkStaging/application_1756131897932_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:24,437 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:24,438 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:24,438 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:24,438 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:24,439 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:24,484 INFO yarn.Client: Submitting application application_1756131897932_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:24,911 INFO impl.YarnClientImpl: Submitted application application_1756131897932_0001\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:25,917 INFO yarn.Client: Application report for application_1756131897932_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:25,922 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Mon Aug 25 14:25:25 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1756131924685\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1756131897932_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:26,926 INFO yarn.Client: Application report for application_1756131897932_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:27,930 INFO yarn.Client: Application report for application_1756131897932_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:28,934 INFO yarn.Client: Application report for application_1756131897932_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:29,945 INFO yarn.Client: Application report for application_1756131897932_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:30,949 INFO yarn.Client: Application report for application_1756131897932_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:31,954 INFO yarn.Client: Application report for application_1756131897932_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:32,959 INFO yarn.Client: Application report for application_1756131897932_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:33,969 INFO yarn.Client: Application report for application_1756131897932_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:34,878 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1756131897932_0001), /proxy/application_1756131897932_0001\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:34,976 INFO yarn.Client: Application report for application_1756131897932_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:34,981 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.184.154\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1756131924685\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1756131897932_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:34,983 INFO cluster.YarnClientSchedulerBackend: Application application_1756131897932_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:35,009 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37715.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:35,010 INFO netty.NettyBlockTransferService: Server created on 10.0.184.154:37715\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:35,012 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:35,023 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.184.154, 37715, None)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:35,041 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.184.154:37715 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.184.154, 37715, None)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:35,056 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.184.154, 37715, None)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:35,065 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.184.154, 37715, None)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:35,367 INFO util.log: Logging initialized @24329ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:37,969 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:44,857 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.184.154:59572) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:45,237 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:34539 with 2.8 GiB RAM, BlockManagerId(1, algo-1, 34539, None)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:47,868 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:48,201 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:48,447 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:48,455 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:50,610 INFO datasources.InMemoryFileIndex: It took 108 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:50,976 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:51,561 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:51,565 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.184.154:37715 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:51,574 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,183 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,193 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,199 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 131771\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,305 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,329 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,330 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,331 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,334 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,345 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,468 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,474 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,475 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.184.154:37715 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,477 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,508 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,509 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,577 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:52,925 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:34539 (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:54,067 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:34539 (size: 39.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:54,604 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2050 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:54,607 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:54,616 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 2.205 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:54,621 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:54,622 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:54,624 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 2.318358 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:54,910 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.184.154:37715 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:54,923 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:34539 in memory (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:58,681 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:58,684 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:58,688 INFO datasources.FileSourceStrategy: Output Data Schema: struct<restingBP: string, serumcholestrol: string, maxheartrate: string, oldpeak: string, gender_0: string ... 24 more fields>\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:58,778 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,100 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,122 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,123 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.184.154:37715 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,125 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,149 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,225 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,227 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,227 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,228 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,231 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,237 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,436 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 20.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,442 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,443 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.184.154:37715 (size: 8.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,444 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,448 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,448 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,454 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:25:59,549 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:34539 (size: 8.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:01,242 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:34539 (size: 39.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:01,699 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:34539 (size: 58.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:02,038 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2588 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:02,039 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:02,040 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.795 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:02,041 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:02,042 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:02,042 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.816679 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:02,680 INFO codegen.CodeGenerator: Code generated in 478.234173 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,805 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,810 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,811 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,811 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,814 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,817 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,845 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 117.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,848 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,849 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.184.154:37715 (size: 35.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,851 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,853 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,854 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,865 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:03,913 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:34539 (size: 35.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,499 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2636 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,502 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 2.681 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,508 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,509 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,509 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,510 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,514 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,701 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,705 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,705 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,705 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,705 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,712 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,750 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 170.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,756 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,757 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.184.154:37715 (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,759 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,760 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,760 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,765 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,804 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:34539 (size: 46.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:06,911 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:07,606 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 842 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:07,606 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:07,610 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.876 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:07,611 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:07,612 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:07,612 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.911105 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:07,710 INFO codegen.CodeGenerator: Code generated in 72.22973 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,228 INFO codegen.CodeGenerator: Code generated in 67.125203 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,331 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,332 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,332 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,332 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,335 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,336 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,368 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 41.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,372 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,375 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.184.154:37715 (size: 17.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,379 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,380 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,380 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,388 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:08,415 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:34539 (size: 17.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:09,064 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 677 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:09,072 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.732 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:09,074 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:09,075 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:09,075 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:09,076 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.744235 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,213 INFO codegen.CodeGenerator: Code generated in 264.145961 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,227 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,228 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,229 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,229 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,230 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,234 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,246 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 76.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,250 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,252 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.184.154:37715 (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,254 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,255 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,256 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,258 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,287 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:34539 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,498 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 241 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,498 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,499 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.261 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,502 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,505 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,505 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,505 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,787 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:34539 in memory (size: 46.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,799 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.184.154:37715 in memory (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,856 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:34539 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,864 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.184.154:37715 in memory (size: 24.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,928 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.184.154:37715 in memory (size: 17.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:10,931 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:34539 in memory (size: 17.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,007 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.184.154:37715 in memory (size: 8.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,008 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:34539 in memory (size: 8.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,065 INFO codegen.CodeGenerator: Code generated in 260.365407 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,091 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:34539 in memory (size: 35.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,124 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.184.154:37715 in memory (size: 35.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,134 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,136 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,137 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,137 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,137 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,138 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,149 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,153 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,156 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.184.154:37715 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,158 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,159 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,160 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,163 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,189 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:34539 (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,218 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,302 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 139 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,302 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,304 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.162 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,310 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,311 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,311 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.176525 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,424 INFO codegen.CodeGenerator: Code generated in 54.120015 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,592 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,599 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,600 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,600 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,601 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,601 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,605 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,623 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 33.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,625 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,628 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.184.154:37715 (size: 15.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,634 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,636 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,636 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,638 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:11,659 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:34539 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,314 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 2676 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,314 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,316 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 2.709 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,317 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,317 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,317 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,317 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,318 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,321 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,324 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,329 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.184.154:37715 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,330 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,331 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,331 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,334 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,370 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:34539 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,378 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,505 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 172 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,506 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.187 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,507 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,507 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,507 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,508 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 2.914911 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,846 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,847 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,847 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,848 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,849 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,852 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,860 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 86.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,862 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,863 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.184.154:37715 (size: 28.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,865 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,866 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,866 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,868 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:14,884 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:34539 (size: 28.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,134 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 266 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,135 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,136 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.283 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,138 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,138 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,138 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,139 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,235 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,245 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,245 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,245 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,246 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,248 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,261 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 171.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,273 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,274 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.184.154:37715 (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,275 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,276 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,276 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,278 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,298 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:34539 (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,324 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,497 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 219 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,498 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,499 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.249 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,500 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,510 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,511 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.267910 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,711 INFO codegen.CodeGenerator: Code generated in 18.518807 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,750 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,752 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,753 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,753 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,754 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,758 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,766 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 41.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,769 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,770 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.184.154:37715 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,771 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,771 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,771 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,773 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,795 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:34539 (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,866 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 93 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,867 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,868 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,871 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,871 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:15,871 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.121024 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,439 INFO codegen.CodeGenerator: Code generated in 148.563984 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,470 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,470 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,470 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,470 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,475 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,476 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,488 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 76.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,490 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,495 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.184.154:37715 (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,496 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,496 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,496 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,501 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,542 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:34539 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,744 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 244 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,745 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,745 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.268 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,746 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,746 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,746 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,746 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,929 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,931 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,932 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,932 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,932 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,933 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,936 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,938 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,939 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.184.154:37715 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,940 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,940 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,940 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,942 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,957 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:34539 (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:16,966 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,001 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 59 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,002 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,003 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.068 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,004 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,005 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,005 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.074797 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,307 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,314 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:34539 in memory (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,329 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,329 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,330 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,330 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,330 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,333 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,337 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.184.154:37715 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,372 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 33.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,378 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,380 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.184.154:37715 (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,381 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,382 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,382 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,388 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,419 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:34539 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,453 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.184.154:37715 in memory (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,459 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:34539 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,506 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 118 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,508 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.173 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,511 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,512 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,513 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,513 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,514 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,515 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:34539 in memory (size: 28.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,511 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,520 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,522 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,526 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.184.154:37715 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,527 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,529 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,530 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,535 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,558 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.184.154:37715 in memory (size: 28.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,573 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:34539 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,580 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,623 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 88 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,623 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,624 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,628 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,630 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,630 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.306895 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,649 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.184.154:37715 in memory (size: 47.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,682 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:34539 in memory (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,741 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.184.154:37715 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,746 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:34539 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,808 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:34539 in memory (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,810 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.184.154:37715 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,888 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.184.154:37715 in memory (size: 17.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:17,888 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:34539 in memory (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,003 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.184.154:37715 in memory (size: 15.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,023 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:34539 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,302 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,303 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,303 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,303 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,306 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,306 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,314 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 86.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,322 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,323 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.184.154:37715 (size: 28.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,325 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,326 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,326 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,328 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,346 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:34539 (size: 28.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,569 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 241 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,570 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,571 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.264 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,572 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,572 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,573 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,573 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,635 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,637 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,638 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,638 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,638 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,640 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,649 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 171.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,658 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,659 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.184.154:37715 (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,660 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,664 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,665 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,666 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,683 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:34539 (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,701 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,829 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 163 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,829 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,830 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.188 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,832 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,833 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,833 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.198037 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:18,955 INFO codegen.CodeGenerator: Code generated in 17.137459 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,006 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,012 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,012 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,013 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,013 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,014 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,024 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 41.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,026 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,027 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.184.154:37715 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,028 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,031 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,032 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,034 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,053 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:34539 (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,142 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 109 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,143 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,144 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.127 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,145 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,145 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,145 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.134485 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,441 INFO codegen.CodeGenerator: Code generated in 78.893587 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,454 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,455 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,455 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,455 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,456 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,456 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,460 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 76.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,464 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,465 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.184.154:37715 (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,472 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,475 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,475 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,477 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,494 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:34539 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,681 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 205 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,681 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,682 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.225 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,683 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,684 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,684 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,684 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,804 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,805 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,806 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,806 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,806 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,807 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,811 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,813 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,814 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.184.154:37715 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,815 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,815 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,816 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,817 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,834 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:34539 (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,840 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,854 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,855 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,856 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.048 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,857 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,858 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,858 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.054093 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,918 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,920 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,920 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,921 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,921 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,921 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,924 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,931 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 33.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,933 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,934 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.184.154:37715 (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,935 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,936 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,936 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,938 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:19,954 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:34539 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,028 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 91 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,029 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,030 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.105 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,031 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,032 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,032 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,032 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,033 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,035 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,038 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,039 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.184.154:37715 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,040 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,040 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,040 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,042 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,060 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:34539 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,065 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,085 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 43 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,085 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,086 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.052 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,088 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,089 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,090 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.170768 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,332 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,333 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,333 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,333 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,334 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,334 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,342 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 86.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,349 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 28.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,350 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.184.154:37715 (size: 28.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,352 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,353 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,358 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,363 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,401 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:34539 (size: 28.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,722 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 359 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,725 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,726 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.388 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,727 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,727 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,727 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,727 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,785 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,787 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,787 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,788 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,788 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,789 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,808 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 171.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,819 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,820 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.184.154:37715 (size: 47.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,820 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,821 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,821 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,824 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,840 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:34539 (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:20,858 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,080 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 257 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,080 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,082 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.292 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,084 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,088 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,092 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.306684 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,450 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,451 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,451 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,451 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,452 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,452 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,472 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 41.0 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,480 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,485 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.184.154:37715 (size: 17.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,491 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,491 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,492 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,494 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,514 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:34539 (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,570 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,570 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,572 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.117 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,572 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,573 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,575 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.124685 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,944 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,946 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,946 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,946 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,947 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,954 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,964 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:34539 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,975 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 76.5 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,976 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.184.154:37715 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,982 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,985 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.184.154:37715 (size: 24.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,986 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,991 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,994 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:21,998 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,058 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:34539 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,099 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.184.154:37715 in memory (size: 28.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,107 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:34539 in memory (size: 28.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,112 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 115 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,112 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,113 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.158 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,113 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,113 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,113 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,113 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,175 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:34539 in memory (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,178 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.184.154:37715 in memory (size: 17.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,240 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:34539 in memory (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,249 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.184.154:37715 in memory (size: 17.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,252 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,253 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,254 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,254 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,256 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,260 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,266 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 66.2 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,269 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.184.154:37715 in memory (size: 28.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,271 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,276 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.184.154:37715 (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,278 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,279 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,279 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,281 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,296 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:34539 in memory (size: 28.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,305 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:34539 (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,326 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,359 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 78 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,360 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,361 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.098 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,362 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,363 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,364 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.111427 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,410 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.184.154:37715 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,414 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:34539 in memory (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,465 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.184.154:37715 in memory (size: 15.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,472 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:34539 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,498 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,500 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,501 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,501 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,501 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,502 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,504 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,517 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:34539 in memory (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,519 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.184.154:37715 in memory (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,555 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.184.154:37715 in memory (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,560 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:34539 in memory (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,573 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 33.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,576 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,577 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.184.154:37715 (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,577 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,578 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,578 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,582 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.184.154:37715 in memory (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,582 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:34539 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,583 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,593 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.184.154:37715 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,596 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:34539 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,606 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:34539 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,632 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:34539 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,635 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.184.154:37715 in memory (size: 24.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,688 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 105 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,688 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,689 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.184 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,690 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,690 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,690 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,690 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,691 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,693 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,697 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,703 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.184.154:37715 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,704 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,704 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,706 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,709 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,729 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:34539 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,738 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,758 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,759 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.066 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,759 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,759 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,759 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,759 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.261291 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,989 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,990 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,990 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,990 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,990 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,991 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,997 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 86.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:22,999 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,000 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.184.154:37715 (size: 28.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,000 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,001 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,001 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,003 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,016 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:34539 (size: 28.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,173 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 170 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,174 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,175 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.183 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,176 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,177 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,177 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,177 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,242 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,243 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,244 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,244 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,244 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,245 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,256 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 171.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,259 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,260 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.184.154:37715 (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,261 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,262 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,262 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,265 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,279 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:34539 (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,292 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,449 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 185 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,449 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,450 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.202 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,450 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,450 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,451 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.209098 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,595 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,596 INFO scheduler.DAGScheduler: Got job 28 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,596 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,597 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,597 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,598 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,606 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 41.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,608 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,609 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.184.154:37715 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,610 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,610 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,610 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,617 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,634 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:34539 (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,689 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 73 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,691 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,692 INFO scheduler.DAGScheduler: ResultStage 41 (treeReduce at KLLRunner.scala:107) finished in 0.093 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,693 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,693 INFO cluster.YarnScheduler: Killing all running tasks in stage 41: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,694 INFO scheduler.DAGScheduler: Job 28 finished: treeReduce at KLLRunner.scala:107, took 0.098587 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,908 INFO scheduler.DAGScheduler: Registering RDD 174 (collect at AnalysisRunner.scala:326) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,909 INFO scheduler.DAGScheduler: Got map stage job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,909 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,909 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,910 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,910 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,916 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 76.5 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,918 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,919 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.184.154:37715 (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,925 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,925 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,926 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,928 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,940 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:34539 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,966 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,966 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,968 INFO scheduler.DAGScheduler: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326) finished in 0.055 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,968 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,969 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,969 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:23,969 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,031 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,032 INFO scheduler.DAGScheduler: Got job 30 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,033 INFO scheduler.DAGScheduler: Final stage: ResultStage 44 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,033 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,033 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,033 INFO scheduler.DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,036 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 66.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,038 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,038 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.184.154:37715 (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,039 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,040 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,040 INFO cluster.YarnScheduler: Adding task set 44.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,042 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,056 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:34539 (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,063 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,072 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 34) in 30 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,072 INFO cluster.YarnScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,073 INFO scheduler.DAGScheduler: ResultStage 44 (collect at AnalysisRunner.scala:326) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,074 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,074 INFO cluster.YarnScheduler: Killing all running tasks in stage 44: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,074 INFO scheduler.DAGScheduler: Job 30 finished: collect at AnalysisRunner.scala:326, took 0.043081 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,196 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,197 INFO scheduler.DAGScheduler: Registering RDD 185 (countByKey at ColumnProfiler.scala:592) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,198 INFO scheduler.DAGScheduler: Got job 31 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,198 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,198 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,198 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,199 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,206 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 33.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,208 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,209 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.184.154:37715 (size: 15.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,211 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,212 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,212 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,213 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,226 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:34539 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,307 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 94 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,308 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,309 INFO scheduler.DAGScheduler: ShuffleMapStage 45 (countByKey at ColumnProfiler.scala:592) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,310 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,312 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,312 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 46)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,312 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,316 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,319 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 5.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,321 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,322 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.184.154:37715 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,323 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,324 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,324 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,325 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 36) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,341 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:34539 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,347 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,359 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 36) in 34 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,359 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,360 INFO scheduler.DAGScheduler: ResultStage 46 (countByKey at ColumnProfiler.scala:592) finished in 0.042 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,361 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,361 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,362 INFO scheduler.DAGScheduler: Job 31 finished: countByKey at ColumnProfiler.scala:592, took 0.165470 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,430 INFO scheduler.DAGScheduler: Registering RDD 191 (collect at AnalysisRunner.scala:326) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,431 INFO scheduler.DAGScheduler: Got map stage job 32 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,431 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,431 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,432 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,432 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,437 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 45.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,439 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,440 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.184.154:37715 (size: 18.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,441 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,441 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,442 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,443 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 37) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,463 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:34539 (size: 18.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,641 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 37) in 198 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,642 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326) finished in 0.208 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,642 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,643 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,643 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,643 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,643 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,688 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,689 INFO scheduler.DAGScheduler: Got job 33 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,689 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,690 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,690 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,690 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,699 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 65.3 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,701 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,702 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.184.154:37715 (size: 24.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,704 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,705 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,705 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,707 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 38) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,721 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:34539 (size: 24.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,735 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,824 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 38) in 117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,825 INFO scheduler.DAGScheduler: ResultStage 49 (collect at AnalysisRunner.scala:326) finished in 0.133 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,825 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,825 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,825 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,826 INFO scheduler.DAGScheduler: Job 33 finished: collect at AnalysisRunner.scala:326, took 0.137367 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,846 INFO codegen.CodeGenerator: Code generated in 17.692583 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,922 INFO codegen.CodeGenerator: Code generated in 22.361717 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:24,998 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,012 INFO scheduler.DAGScheduler: Got job 34 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,012 INFO scheduler.DAGScheduler: Final stage: ResultStage 50 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,012 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,013 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,013 INFO scheduler.DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,019 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 36.4 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,025 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,026 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.184.154:37715 (size: 16.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,026 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,031 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,031 INFO cluster.YarnScheduler: Adding task set 50.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,040 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 39) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,053 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-1:34539 (size: 16.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,123 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 39) in 83 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,124 INFO scheduler.DAGScheduler: ResultStage 50 (treeReduce at KLLRunner.scala:107) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,125 INFO scheduler.DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,125 INFO cluster.YarnScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,126 INFO cluster.YarnScheduler: Killing all running tasks in stage 50: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,126 INFO scheduler.DAGScheduler: Job 34 finished: treeReduce at KLLRunner.scala:107, took 0.118602 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,326 INFO codegen.CodeGenerator: Code generated in 49.608216 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,337 INFO scheduler.DAGScheduler: Registering RDD 209 (collect at AnalysisRunner.scala:326) as input to shuffle 16\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,337 INFO scheduler.DAGScheduler: Got map stage job 35 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,338 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,338 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,338 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,339 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,344 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 35.8 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,388 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 14.5 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,392 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.184.154:37715 (size: 14.5 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,395 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,401 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,401 INFO cluster.YarnScheduler: Adding task set 51.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,402 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.0.184.154:37715 in memory (size: 18.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,404 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 40) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,404 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on algo-1:34539 in memory (size: 18.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,410 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.184.154:37715 in memory (size: 28.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,421 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:34539 in memory (size: 28.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,427 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-1:34539 (size: 14.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,436 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.0.184.154:37715 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,457 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-1:34539 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,462 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.184.154:37715 in memory (size: 47.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,464 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:34539 in memory (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,471 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.184.154:37715 in memory (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,473 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:34539 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,478 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.0.184.154:37715 in memory (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,484 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-1:34539 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,489 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.0.184.154:37715 in memory (size: 16.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,493 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-1:34539 in memory (size: 16.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,499 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.184.154:37715 in memory (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,505 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:34539 in memory (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,510 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.0.184.154:37715 in memory (size: 24.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,518 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-1:34539 in memory (size: 24.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,528 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.184.154:37715 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,532 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:34539 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,540 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.184.154:37715 in memory (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,545 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:34539 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,552 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.184.154:37715 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,553 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:34539 in memory (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,557 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.0.184.154:37715 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,572 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:34539 in memory (size: 19.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,577 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.0.184.154:37715 in memory (size: 24.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,582 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:34539 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,583 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 40) in 179 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,583 INFO cluster.YarnScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,584 INFO scheduler.DAGScheduler: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326) finished in 0.245 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,584 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,584 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,584 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,584 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,667 INFO codegen.CodeGenerator: Code generated in 35.876635 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,695 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,696 INFO scheduler.DAGScheduler: Got job 36 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,697 INFO scheduler.DAGScheduler: Final stage: ResultStage 53 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,697 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,697 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,698 INFO scheduler.DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,704 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 21.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,706 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,707 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.184.154:37715 (size: 8.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,707 INFO spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,708 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,708 INFO cluster.YarnScheduler: Adding task set 53.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,716 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 41) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,732 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-1:34539 (size: 8.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,737 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,799 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 41) in 83 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,799 INFO cluster.YarnScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,800 INFO scheduler.DAGScheduler: ResultStage 53 (collect at AnalysisRunner.scala:326) finished in 0.100 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,801 INFO scheduler.DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,801 INFO cluster.YarnScheduler: Killing all running tasks in stage 53: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,802 INFO scheduler.DAGScheduler: Job 36 finished: collect at AnalysisRunner.scala:326, took 0.106527 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,843 INFO codegen.CodeGenerator: Code generated in 36.944187 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,924 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,925 INFO scheduler.DAGScheduler: Registering RDD 220 (countByKey at ColumnProfiler.scala:592) as input to shuffle 17\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,925 INFO scheduler.DAGScheduler: Got job 37 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,925 INFO scheduler.DAGScheduler: Final stage: ResultStage 55 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,926 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,926 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,927 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,934 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,937 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,938 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.184.154:37715 (size: 14.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,938 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,939 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,939 INFO cluster.YarnScheduler: Adding task set 54.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,942 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 42) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:25,955 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-1:34539 (size: 14.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,034 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 42) in 92 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,035 INFO cluster.YarnScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,036 INFO scheduler.DAGScheduler: ShuffleMapStage 54 (countByKey at ColumnProfiler.scala:592) finished in 0.108 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,037 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,037 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,037 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 55)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,037 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,038 INFO scheduler.DAGScheduler: Submitting ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,040 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,041 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,042 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.184.154:37715 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,042 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,043 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,043 INFO cluster.YarnScheduler: Adding task set 55.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,047 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 55.0 (TID 43) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,060 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-1:34539 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,064 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,080 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 55.0 (TID 43) in 34 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,080 INFO cluster.YarnScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,081 INFO scheduler.DAGScheduler: ResultStage 55 (countByKey at ColumnProfiler.scala:592) finished in 0.042 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,082 INFO scheduler.DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,082 INFO cluster.YarnScheduler: Killing all running tasks in stage 55: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,082 INFO scheduler.DAGScheduler: Job 37 finished: countByKey at ColumnProfiler.scala:592, took 0.158697 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,418 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,466 INFO codegen.CodeGenerator: Code generated in 11.838077 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,480 INFO scheduler.DAGScheduler: Registering RDD 226 (count at StatsGenerator.scala:66) as input to shuffle 18\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,481 INFO scheduler.DAGScheduler: Got map stage job 38 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,481 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 56 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,481 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,482 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,483 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[226] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,486 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 25.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,488 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,489 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.184.154:37715 (size: 11.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,489 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,490 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[226] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,490 INFO cluster.YarnScheduler: Adding task set 56.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,497 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 56.0 (TID 44) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,508 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-1:34539 (size: 11.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,547 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 56.0 (TID 44) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,548 INFO cluster.YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,549 INFO scheduler.DAGScheduler: ShuffleMapStage 56 (count at StatsGenerator.scala:66) finished in 0.066 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,549 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,549 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,550 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,550 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,567 INFO codegen.CodeGenerator: Code generated in 9.945394 ms\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,582 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,591 INFO scheduler.DAGScheduler: Got job 39 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,592 INFO scheduler.DAGScheduler: Final stage: ResultStage 58 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,592 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,592 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,593 INFO scheduler.DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[229] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,595 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 11.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,597 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,597 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.184.154:37715 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,598 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,598 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[229] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,598 INFO cluster.YarnScheduler: Adding task set 58.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,600 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 58.0 (TID 45) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,610 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on algo-1:34539 (size: 5.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,615 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.184.154:59572\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,654 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 58.0 (TID 45) in 55 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,655 INFO cluster.YarnScheduler: Removed TaskSet 58.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,656 INFO scheduler.DAGScheduler: ResultStage 58 (count at StatsGenerator.scala:66) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,656 INFO scheduler.DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,657 INFO cluster.YarnScheduler: Killing all running tasks in stage 58: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:26,657 INFO scheduler.DAGScheduler: Job 39 finished: count at StatsGenerator.scala:66, took 0.066411 s\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,201 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,220 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,301 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,303 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,316 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,362 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,407 WARN nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@7459f128.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,408 INFO nio.NioEventLoop: Migrated 1 channel(s) to the new Selector.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,431 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,445 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,456 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,466 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,513 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,513 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,514 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,544 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,545 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-166f308e-5397-42b7-bafb-fac2cef1f702\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,574 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-95431795-e976-4fd8-a1ef-4a1c568c83b7\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,751 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-08-25 14:26:27,752 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'iti113-team2-bucket'\n",
    "base_folder = 'Team2'\n",
    "\n",
    "monitor_output_path = f\"s3://{bucket_name}/{base_folder}/monitoring\"\n",
    "\n",
    "# Create a DefaultModelMonitor instance\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.large',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Run the baseline job\n",
    "baseline_data = f's3://{bucket_name}/{base_folder}/processing/train/v1/train.csv'\n",
    "baseline_job = monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data,\n",
    "    dataset_format={'csv': {'header': True}},\n",
    "    output_s3_uri=f\"{monitor_output_path}/baseline\",\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48dfd0-4694-4556-9e6b-06c2c25ea355",
   "metadata": {},
   "source": [
    "------\n",
    "#### Re-attach to the job as a ProcessingJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a75536c2-d6a7-45ed-9cd9-ad1cf060db9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:21:32.239768Z",
     "iopub.status.busy": "2025-08-25T15:21:32.239370Z",
     "iopub.status.idle": "2025-08-25T15:21:32.465093Z",
     "shell.execute_reply": "2025-08-25T15:21:32.464331Z",
     "shell.execute_reply.started": "2025-08-25T15:21:32.239742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully re-attached to job: baseline-suggestion-job-2025-08-25-14-21-55-214\n",
      "\n",
      "Loading constraints from: s3://iti113-team2-bucket/Team2/monitoring/baseline/constraints.json\n",
      "\n",
      "--- Sample of Generated Constraints ---\n",
      "{'features': [{'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'restingBP',\n",
      "               'num_constraints': {'is_non_negative': False}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'serumcholestrol',\n",
      "               'num_constraints': {'is_non_negative': False}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'maxheartrate',\n",
      "               'num_constraints': {'is_non_negative': False}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'oldpeak',\n",
      "               'num_constraints': {'is_non_negative': False}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'gender_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'gender_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'chestpain_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'chestpain_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'chestpain_2',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'chestpain_3',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'fastingbloodsugar_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'fastingbloodsugar_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'restingrelectro_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'restingrelectro_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'restingrelectro_2',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'exerciseangia_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'exerciseangia_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'slope_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'slope_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'slope_2',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'slope_3',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'noofmajorvessels_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'noofmajorvessels_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'noofmajorvessels_2',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'noofmajorvessels_3',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Integral',\n",
      "               'name': 'target',\n",
      "               'num_constraints': {'is_non_negative': True}}],\n",
      " 'monitoring_config': {'datatype_check_threshold': 1.0,\n",
      "                       'distribution_constraints': {'categorical_comparison_threshold': 0.1,\n",
      "                                                    'categorical_drift_method': 'LInfinity',\n",
      "                                                    'comparison_method': 'Robust',\n",
      "                                                    'comparison_threshold': 0.1,\n",
      "                                                    'perform_comparison': 'Enabled'},\n",
      "                       'domain_content_threshold': 1.0,\n",
      "                       'emit_metrics': 'Enabled',\n",
      "                       'evaluate_constraints': 'Enabled'},\n",
      " 'version': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingJob\n",
    "from sagemaker.model_monitor import Constraints\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "baseline_job = None\n",
    "# !!!! The job name from your successful run\n",
    "baseline_job_name = \"baseline-suggestion-job-2025-08-25-14-21-55-214\"\n",
    "\n",
    "try:\n",
    "    # Re-attach as a ProcessingJob\n",
    "    baseline_job = ProcessingJob.from_processing_name(\n",
    "        processing_job_name=baseline_job_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    print(f\"Successfully re-attached to job: {baseline_job_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to re-attach to the job. Error: {e}\")\n",
    "\n",
    "\n",
    "# Now, load and inspect the constraints from S3\n",
    "if baseline_job:\n",
    "    # Find the output configuration in the job description\n",
    "    output_config = baseline_job.describe()['ProcessingOutputConfig']['Outputs']\n",
    "\n",
    "    # Find the S3 URI for the baseline output\n",
    "    baseline_output_uri = None\n",
    "    for output in output_config:\n",
    "        if output['OutputName'] == 'monitoring_output':\n",
    "            baseline_output_uri = output['S3Output']['S3Uri']\n",
    "            break\n",
    "\n",
    "    if baseline_output_uri:\n",
    "        # Construct the full path to the constraints file\n",
    "        constraints_s3_uri = f\"{baseline_output_uri}/constraints.json\"\n",
    "        print(f\"\\nLoading constraints from: {constraints_s3_uri}\")\n",
    "\n",
    "        # Load the constraints file from S3\n",
    "        suggested_constraints = Constraints.from_s3_uri(constraints_s3_uri)\n",
    "\n",
    "        # Now print the dictionary, just like in the other example\n",
    "        print(\"\\n--- Sample of Generated Constraints ---\")\n",
    "        from pprint import pprint\n",
    "        pprint(suggested_constraints.body_dict)\n",
    "        # ===================================================================\n",
    "    else:\n",
    "        print(\"Could not find the baseline output path in the job description.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e525b11-c6ad-4e2a-b1b2-9464a9ec35c1",
   "metadata": {},
   "source": [
    "----\n",
    "#### Schedule the Monitoring Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "197b14dc-abde-446b-9711-fc4c3e3178ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:27:29.366627Z",
     "iopub.status.busy": "2025-08-25T15:27:29.366055Z",
     "iopub.status.idle": "2025-08-25T15:27:30.409396Z",
     "shell.execute_reply": "2025-08-25T15:27:30.408673Z",
     "shell.execute_reply.started": "2025-08-25T15:27:29.366598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint status: InService\n",
      "SageMaker default region: ap-southeast-1\n",
      "No schedule named 'Team2-drift-schedule-main' found. Creating a new one.\n",
      "Monitoring schedule 'Team2-drift-schedule-main' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator, DefaultModelMonitor\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\", region_name=\"ap-southeast-1\")\n",
    "\n",
    "# Confirm endpoint is InService\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=\"Team2-predictor-endpoint\")\n",
    "print(\"Endpoint status:\", response['EndpointStatus'])  # should be \"InService\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "print(\"SageMaker default region:\", sagemaker_session.boto_region_name)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket_name = 'iti113-team2-bucket'\n",
    "base_folder = 'Team2'\n",
    "\n",
    "monitor_output_path = f\"s3://{bucket_name}/{base_folder}/monitoring\"\n",
    "baseline_output_uri = f\"{monitor_output_path}/baseline\"\n",
    "\n",
    "#schedule_name\n",
    "schedule_name = \"Team2-drift-schedule-main\"\n",
    "\n",
    "# Initialize model monitor\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.large',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Check if schedule exists\n",
    "    sagemaker_client.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "    print(f\"Found existing monitoring schedule: '{schedule_name}'\")\n",
    "\n",
    "    # Attach monitor to the schedule\n",
    "    monitor.attach(schedule_name)\n",
    "    monitor.monitoring_schedule_name = schedule_name\n",
    "    print(f\"Successfully attached monitor object to the schedule.\")\n",
    "\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceNotFound':\n",
    "        print(f\"No schedule named '{schedule_name}' found. Creating a new one.\")\n",
    "\n",
    "        endpoint_input = EndpointInput(\n",
    "            endpoint_name=\"Team2-predictor-endpoint\",\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        )\n",
    "        \n",
    "        # Create monitoring schedule\n",
    "        monitor.create_monitoring_schedule(\n",
    "            monitor_schedule_name=schedule_name,\n",
    "            endpoint_input=endpoint_input,  # just pass the name directly\n",
    "            output_s3_uri=f\"{monitor_output_path}/reports\",\n",
    "            statistics=f\"{baseline_output_uri}/statistics.json\",\n",
    "            constraints=f\"{baseline_output_uri}/constraints.json\",\n",
    "            schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "            enable_cloudwatch_metrics=True,\n",
    "        )\n",
    "        print(f\"Monitoring schedule '{schedule_name}' created successfully.\")\n",
    "        \n",
    "        # Attach after creation\n",
    "        # monitor.attach(monitoring_schedule_name=schedule_name)\n",
    "        \n",
    "    else:\n",
    "        print(\"An unexpected error occurred while checking for the schedule.\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2fb52bf-57a6-473e-9372-886fa38d5177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:27:48.725717Z",
     "iopub.status.busy": "2025-08-25T15:27:48.725423Z",
     "iopub.status.idle": "2025-08-25T15:27:48.777610Z",
     "shell.execute_reply": "2025-08-25T15:27:48.776912Z",
     "shell.execute_reply.started": "2025-08-25T15:27:48.725695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Scheduled\n"
     ]
    }
   ],
   "source": [
    "# Now describe schedule details\n",
    "try:\n",
    "    schedule_details = monitor.describe_schedule()\n",
    "    print(f\"Schedule status: {schedule_details['MonitoringScheduleStatus']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve schedule details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc4737-f839-4def-9ff6-846247a63d39",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Simulate, Detect, and Analyze Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42f394ba-7241-4c60-8333-ea95660f95e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:27:54.563285Z",
     "iopub.status.busy": "2025-08-25T15:27:54.562465Z",
     "iopub.status.idle": "2025-08-25T15:28:08.302449Z",
     "shell.execute_reply": "2025-08-25T15:28:08.301696Z",
     "shell.execute_reply.started": "2025-08-25T15:27:54.563255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original average restingBP: 0\n",
      "New average age: 1.0\n",
      "\n",
      "Number of features: 25\n",
      "\n",
      "Sending 100 drifted requests to endpoint: Team2-predictor-endpoint\n",
      "[0] ✅ Response: {'predictions': [1]}\n",
      "[1] ✅ Response: {'predictions': [0]}\n",
      "[2] ✅ Response: {'predictions': [1]}\n",
      "[3] ✅ Response: {'predictions': [0]}\n",
      "[4] ✅ Response: {'predictions': [1]}\n",
      "[5] ✅ Response: {'predictions': [0]}\n",
      "[6] ✅ Response: {'predictions': [0]}\n",
      "[7] ✅ Response: {'predictions': [1]}\n",
      "[8] ✅ Response: {'predictions': [1]}\n",
      "[9] ✅ Response: {'predictions': [1]}\n",
      "[10] ✅ Response: {'predictions': [0]}\n",
      "[11] ✅ Response: {'predictions': [1]}\n",
      "[12] ✅ Response: {'predictions': [1]}\n",
      "[13] ✅ Response: {'predictions': [1]}\n",
      "[14] ✅ Response: {'predictions': [1]}\n",
      "[15] ✅ Response: {'predictions': [1]}\n",
      "[16] ✅ Response: {'predictions': [1]}\n",
      "[17] ✅ Response: {'predictions': [1]}\n",
      "[18] ✅ Response: {'predictions': [0]}\n",
      "[19] ✅ Response: {'predictions': [1]}\n",
      "[20] ✅ Response: {'predictions': [1]}\n",
      "[21] ✅ Response: {'predictions': [1]}\n",
      "[22] ✅ Response: {'predictions': [0]}\n",
      "[23] ✅ Response: {'predictions': [0]}\n",
      "[24] ✅ Response: {'predictions': [0]}\n",
      "[25] ✅ Response: {'predictions': [1]}\n",
      "[26] ✅ Response: {'predictions': [0]}\n",
      "[27] ✅ Response: {'predictions': [1]}\n",
      "[28] ✅ Response: {'predictions': [0]}\n",
      "[29] ✅ Response: {'predictions': [1]}\n",
      "[30] ✅ Response: {'predictions': [1]}\n",
      "[31] ✅ Response: {'predictions': [0]}\n",
      "[32] ✅ Response: {'predictions': [1]}\n",
      "[33] ✅ Response: {'predictions': [1]}\n",
      "[34] ✅ Response: {'predictions': [1]}\n",
      "[35] ✅ Response: {'predictions': [1]}\n",
      "[36] ✅ Response: {'predictions': [1]}\n",
      "[37] ✅ Response: {'predictions': [0]}\n",
      "[38] ✅ Response: {'predictions': [1]}\n",
      "[39] ✅ Response: {'predictions': [0]}\n",
      "[40] ✅ Response: {'predictions': [0]}\n",
      "[41] ✅ Response: {'predictions': [1]}\n",
      "[42] ✅ Response: {'predictions': [0]}\n",
      "[43] ✅ Response: {'predictions': [1]}\n",
      "[44] ✅ Response: {'predictions': [0]}\n",
      "[45] ✅ Response: {'predictions': [0]}\n",
      "[46] ✅ Response: {'predictions': [1]}\n",
      "[47] ✅ Response: {'predictions': [1]}\n",
      "[48] ✅ Response: {'predictions': [1]}\n",
      "[49] ✅ Response: {'predictions': [1]}\n",
      "[50] ✅ Response: {'predictions': [0]}\n",
      "[51] ✅ Response: {'predictions': [1]}\n",
      "[52] ✅ Response: {'predictions': [1]}\n",
      "[53] ✅ Response: {'predictions': [0]}\n",
      "[54] ✅ Response: {'predictions': [1]}\n",
      "[55] ✅ Response: {'predictions': [0]}\n",
      "[56] ✅ Response: {'predictions': [1]}\n",
      "[57] ✅ Response: {'predictions': [1]}\n",
      "[58] ✅ Response: {'predictions': [1]}\n",
      "[59] ✅ Response: {'predictions': [0]}\n",
      "[60] ✅ Response: {'predictions': [0]}\n",
      "[61] ✅ Response: {'predictions': [1]}\n",
      "[62] ✅ Response: {'predictions': [0]}\n",
      "[63] ✅ Response: {'predictions': [1]}\n",
      "[64] ✅ Response: {'predictions': [0]}\n",
      "[65] ✅ Response: {'predictions': [0]}\n",
      "[66] ✅ Response: {'predictions': [1]}\n",
      "[67] ✅ Response: {'predictions': [1]}\n",
      "[68] ✅ Response: {'predictions': [1]}\n",
      "[69] ✅ Response: {'predictions': [0]}\n",
      "[70] ✅ Response: {'predictions': [0]}\n",
      "[71] ✅ Response: {'predictions': [1]}\n",
      "[72] ✅ Response: {'predictions': [1]}\n",
      "[73] ✅ Response: {'predictions': [1]}\n",
      "[74] ✅ Response: {'predictions': [0]}\n",
      "[75] ✅ Response: {'predictions': [1]}\n",
      "[76] ✅ Response: {'predictions': [1]}\n",
      "[77] ✅ Response: {'predictions': [0]}\n",
      "[78] ✅ Response: {'predictions': [1]}\n",
      "[79] ✅ Response: {'predictions': [0]}\n",
      "[80] ✅ Response: {'predictions': [1]}\n",
      "[81] ✅ Response: {'predictions': [1]}\n",
      "[82] ✅ Response: {'predictions': [0]}\n",
      "[83] ✅ Response: {'predictions': [1]}\n",
      "[84] ✅ Response: {'predictions': [0]}\n",
      "[85] ✅ Response: {'predictions': [1]}\n",
      "[86] ✅ Response: {'predictions': [1]}\n",
      "[87] ✅ Response: {'predictions': [0]}\n",
      "[88] ✅ Response: {'predictions': [0]}\n",
      "[89] ✅ Response: {'predictions': [1]}\n",
      "[90] ✅ Response: {'predictions': [1]}\n",
      "[91] ✅ Response: {'predictions': [1]}\n",
      "[92] ✅ Response: {'predictions': [0]}\n",
      "[93] ✅ Response: {'predictions': [0]}\n",
      "[94] ✅ Response: {'predictions': [1]}\n",
      "[95] ✅ Response: {'predictions': [1]}\n",
      "[96] ✅ Response: {'predictions': [1]}\n",
      "[97] ✅ Response: {'predictions': [1]}\n",
      "[98] ✅ Response: {'predictions': [0]}\n",
      "[99] ✅ Response: {'predictions': [1]}\n",
      "\n",
      "✅ All drifted requests sent.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "bucket_name = 'iti113-team2-bucket'\n",
    "base_folder = 'Team2'\n",
    "endpoint_name = \"Team2-predictor-endpoint\"\n",
    "aws_region = \"ap-southeast-1\"\n",
    "\n",
    "# === SETUP CLIENTS ===\n",
    "boto_session = boto3.Session(region_name=aws_region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "sagemaker_client = boto3.client(\"sagemaker\", region_name=aws_region)\n",
    "s3_client = boto3.client('s3', region_name=aws_region)\n",
    "\n",
    "s3_process_test_path = f\"s3://{bucket_name}/{base_folder}/processing/test/v1/test.csv\"\n",
    "df = pd.read_csv(s3_process_test_path)\n",
    "df = df.drop(\"target\", axis=1)\n",
    "\n",
    "# Select a few rows to manipulate (simulate drift)\n",
    "# drifted_data = df.head(5).copy()\n",
    "drifted_data = df.head(100).copy()\n",
    "\n",
    "# Manually introduce DRIFT\n",
    "print(\"Original average restingBP:\", int(drifted_data['restingBP'].mean()))\n",
    "drifted_data['restingBP'] = 1\n",
    "print(\"New average age:\", drifted_data['restingBP'].mean())\n",
    "\n",
    "# 2. Categorical Drift: \n",
    "# if 'restingelectro' in drifted_data.columns:\n",
    "#     drifted_data['restingelectro'] = 1  # Assuming 1 maps to 'ST-abnormality'\n",
    "#     print(\"\\n'restingelectro' distribution changed to single category 'ST-abnormality'.\")\n",
    "# else:\n",
    "#     print(\"\\nWarning: 'restingelectro' column not found.\")\n",
    "\n",
    "# Print sample payloads\n",
    "# print(\"\\nSample drifted payloads:\")\n",
    "# print(drifted_data.head())\n",
    "\n",
    "print(\"\\nNumber of features:\", drifted_data.shape[1])\n",
    "\n",
    "# Create SageMaker predictor\n",
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# === SEND PREDICTION PAYLOADS ===\n",
    "print(f\"\\nSending {len(drifted_data)} drifted requests to endpoint: {endpoint_name}\")\n",
    "for i, row in drifted_data.iterrows():\n",
    "    payload = {\"data\": [row.to_dict()]}\n",
    "    try:\n",
    "        response = predictor.predict(payload)\n",
    "        print(f\"[{i}] ✅ Response: {response}\")\n",
    "        time.sleep(0.1)  # Optional delay to avoid throttling\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}] ❌ Error sending request: {e}\")\n",
    "\n",
    "print(\"\\n✅ All drifted requests sent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95aa035-3c1a-4d98-8a20-7f36c63f063f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T13:14:53.670079Z",
     "iopub.status.busy": "2025-08-25T13:14:53.669328Z",
     "iopub.status.idle": "2025-08-25T13:14:53.764248Z",
     "shell.execute_reply": "2025-08-25T13:14:53.763450Z",
     "shell.execute_reply.started": "2025-08-25T13:14:53.670050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Manual monitoring job started for schedule: 'Team2-drift-schedule-main' at 2025-08-25-13-14-53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2076/2497999325.py:7: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow().strftime('%Y-%m-%d-%H-%M-%S')\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "# import boto3\n",
    "\n",
    "# client = boto3.client(\"sagemaker\", region_name=\"ap-southeast-1\")\n",
    "\n",
    "# # Optional: give the job a timestamped name\n",
    "# current_time = datetime.utcnow().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "# # Trigger a monitoring job manually\n",
    "# response = client.start_monitoring_schedule(\n",
    "#     MonitoringScheduleName=schedule_name\n",
    "# )\n",
    "\n",
    "# print(f\"✅ Manual monitoring job started for schedule: '{schedule_name}' at {current_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ffb573f-ba9d-4d3b-b2c6-d9d9af2bc0ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:33:25.798215Z",
     "iopub.status.busy": "2025-08-25T15:33:25.797563Z",
     "iopub.status.idle": "2025-08-25T15:33:25.881344Z",
     "shell.execute_reply": "2025-08-25T15:33:25.880587Z",
     "shell.execute_reply.started": "2025-08-25T15:33:25.798187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ContentType: application/json\n",
      "{\"predictions\": [1, 0]}\n"
     ]
    }
   ],
   "source": [
    "#test invoke endpoint\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\", region_name=aws_region)\n",
    "\n",
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps({\"data\": drifted_data.head(2).to_dict(orient=\"records\")})\n",
    ")\n",
    "print(f\"\\nContentType: {response[\"ContentType\"]}\")\n",
    "print(response[\"Body\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4841fb-e4ba-414e-b9c4-b414394e1fe4",
   "metadata": {},
   "source": [
    "----\n",
    "#### Manual start monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75a6a901-ce67-4ea6-8ef1-3992797fe9a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:44:35.245482Z",
     "iopub.status.busy": "2025-08-25T15:44:35.245172Z",
     "iopub.status.idle": "2025-08-25T15:44:35.329532Z",
     "shell.execute_reply": "2025-08-25T15:44:35.328789Z",
     "shell.execute_reply.started": "2025-08-25T15:44:35.245460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'fcf93ee5-3104-49a2-a816-4803f425399a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'fcf93ee5-3104-49a2-a816-4803f425399a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Mon, 25 Aug 2025 15:44:35 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_client.start_monitoring_schedule(MonitoringScheduleName=\"Team2-drift-schedule-main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01f2aa5e-4454-4a68-90f7-898a72879395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:45:44.112612Z",
     "iopub.status.busy": "2025-08-25T15:45:44.112052Z",
     "iopub.status.idle": "2025-08-25T15:45:44.186312Z",
     "shell.execute_reply": "2025-08-25T15:45:44.185676Z",
     "shell.execute_reply.started": "2025-08-25T15:45:44.112578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule Status: Scheduled\n",
      "Latest Execution Status: Failed\n",
      "Failure Reason: AlgorithmError: Error: Encoding mismatch: Encoding is JSON for endpointInput, but Encoding is BASE64 for endpointOutput. We currently only support the same type of input and output encoding at the moment., exit code: 255\n"
     ]
    }
   ],
   "source": [
    "response = sagemaker_client.describe_monitoring_schedule(\n",
    "    MonitoringScheduleName=\"Team2-drift-schedule-main\"\n",
    ")\n",
    "\n",
    "summary = response.get(\"LastMonitoringExecutionSummary\", {})\n",
    "print(\"Schedule Status:\", response.get(\"MonitoringScheduleStatus\"))\n",
    "print(\"Latest Execution Status:\", summary.get(\"MonitoringExecutionStatus\"))\n",
    "if summary.get(\"FailureReason\"):\n",
    "    print(\"Failure Reason:\", summary[\"FailureReason\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "075d7754-8bfc-456b-aa56-e13a4e616063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:45:45.599336Z",
     "iopub.status.busy": "2025-08-25T15:45:45.598433Z",
     "iopub.status.idle": "2025-08-25T15:45:45.602766Z",
     "shell.execute_reply": "2025-08-25T15:45:45.602028Z",
     "shell.execute_reply.started": "2025-08-25T15:45:45.599303Z"
    }
   },
   "outputs": [],
   "source": [
    "# monitoring_jobs = sagemaker_session.list_monitoring_executions(monitoring_schedule_name=\"Team2-drift-schedule-main\")\n",
    "# print(monitoring_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6ad2d-0d26-4b1d-91d5-46e50330fa89",
   "metadata": {},
   "source": [
    "----\n",
    "#### Check scheduled monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04e678a4-a5fc-4c9a-9b15-c5ebb63ec7dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:45:46.611099Z",
     "iopub.status.busy": "2025-08-25T15:45:46.610754Z",
     "iopub.status.idle": "2025-08-25T15:45:46.853560Z",
     "shell.execute_reply": "2025-08-25T15:45:46.852658Z",
     "shell.execute_reply.started": "2025-08-25T15:45:46.611073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule Name: Team2-drift-schedule-main\n",
      "Schedule Status: Scheduled\n",
      "Endpoint Name: Team2-predictor-endpoint\n",
      "Latest Execution Status: Failed\n",
      "FailureReasons: AlgorithmError: Error: Encoding mismatch: Encoding is JSON for endpointInput, but Encoding is BASE64 for endpointOutput. We currently only support the same type of input and output encoding at the moment., exit code: 255\n",
      "Monitoring job did not complete successfully. Current status: Failed\n"
     ]
    }
   ],
   "source": [
    "#Alternative: read from schedule monitoring\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "from sagemaker.model_monitor import MonitoringExecution\n",
    "\n",
    "# Check Schedule and Execution Status\n",
    "desc = monitor.describe_schedule()\n",
    "print(f\"Schedule Name: {desc['MonitoringScheduleName']}\")\n",
    "print(f\"Schedule Status: {desc['MonitoringScheduleStatus']}\")\n",
    "print(f\"Endpoint Name: {desc['EndpointName']}\")\n",
    "\n",
    "executions = monitor.list_executions()\n",
    "if executions:\n",
    "    latest_execution: MonitoringExecution = executions[0]\n",
    "    status = latest_execution.describe()['ProcessingJobStatus']\n",
    "    print(f\"Latest Execution Status: {status}\")\n",
    "\n",
    "    FailureReason = latest_execution.describe()['FailureReason']\n",
    "    print(f\"FailureReasons: {FailureReason}\")\n",
    "\n",
    "    # Check for Completion and Get Report\n",
    "    if status in ['CompletedWithViolations', 'Completed']:\n",
    "\n",
    "        try:\n",
    "            # Get the report object from the SDK\n",
    "            violations_report = latest_execution.constraint_violations()\n",
    "\n",
    "            if violations_report:\n",
    "                violations_list = violations_report.body_dict.get(\"violations\", [])\n",
    "\n",
    "                if violations_list:\n",
    "                    print(\"\\nDRIFT DETECTED! Details:\")\n",
    "                    for v in violations_list:\n",
    "                        print(f\" - {v['feature_name']}: {v['description']}\")\n",
    "                else:\n",
    "                    # This handles the case where the report exists but is empty\n",
    "                    print(\"\\nNo violations found in the report body.\")\n",
    "            else:\n",
    "                # This handles when the job completes but no violations file is found at all\n",
    "                print(\"\\nNo violations found. The job completed successfully without detecting drift.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # This is a fallback for other potential errors\n",
    "            print(f\"\\nAn error occurred while trying to retrieve the report: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Monitoring job did not complete successfully. Current status: {status}\")\n",
    "else:\n",
    "    print(\"No monitoring jobs have run yet. Please wait and re-run later.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8dae90-5298-478b-ae2b-b49c06307004",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Check if DataCaptureConfig enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9cce1c62-9a3e-4aef-b432-fef2ef4ad1c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:45:52.752558Z",
     "iopub.status.busy": "2025-08-25T15:45:52.751866Z",
     "iopub.status.idle": "2025-08-25T15:45:52.937211Z",
     "shell.execute_reply": "2025-08-25T15:45:52.936349Z",
     "shell.execute_reply.started": "2025-08-25T15:45:52.752528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data capture is enabled.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "endpoint_name = \"Team2-predictor-endpoint\"\n",
    "\n",
    "# Get endpoint config name\n",
    "endpoint_response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_config_name = endpoint_response[\"EndpointConfigName\"]\n",
    "\n",
    "config_response = sagemaker_client.describe_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "capture_config = config_response.get(\"DataCaptureConfig\")\n",
    "\n",
    "if capture_config and capture_config.get(\"EnableCapture\"):\n",
    "    print(\"✅ Data capture is enabled.\")\n",
    "else:\n",
    "    print(\"❌ Data capture is NOT enabled. Monitoring cannot be scheduled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8246c5-a25e-4699-8e39-4759c8d72a9d",
   "metadata": {},
   "source": [
    "#### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "401f102a-62fa-4a1c-939b-967be8696060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:46:00.976690Z",
     "iopub.status.busy": "2025-08-25T15:46:00.976112Z",
     "iopub.status.idle": "2025-08-25T15:46:11.383942Z",
     "shell.execute_reply": "2025-08-25T15:46:11.381577Z",
     "shell.execute_reply.started": "2025-08-25T15:46:00.976661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring schedule 'Team2-drift-schedule-main' deleted.\n"
     ]
    }
   ],
   "source": [
    "# Clean up resources\n",
    "try:\n",
    "    # Delete the Monitoring Schedule\n",
    "    monitor.delete_monitoring_schedule()\n",
    "    print(f\"Monitoring schedule '{schedule_name}' deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not delete schedule: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
