{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3e9b31d1-6843-47ad-b2bb-fb91f27e4578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:14:56.663439Z",
     "iopub.status.busy": "2025-08-27T05:14:56.663093Z",
     "iopub.status.idle": "2025-08-27T05:14:56.725599Z",
     "shell.execute_reply": "2025-08-27T05:14:56.723802Z",
     "shell.execute_reply.started": "2025-08-27T05:14:56.663415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::837028399719:role/iti113-team2-sagemaker-iti113-team2-domain-iti113-team2-Role\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "# Setup SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014517f-019a-47c6-b421-bc863048ffd6",
   "metadata": {},
   "source": [
    "----\n",
    "#### Generate the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e105b08-6d8d-4eab-b212-9b4c1fa2b4f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T02:24:50.786604Z",
     "iopub.status.busy": "2025-08-27T02:24:50.785927Z",
     "iopub.status.idle": "2025-08-27T02:32:45.385363Z",
     "shell.execute_reply": "2025-08-27T02:32:45.384511Z",
     "shell.execute_reply.started": "2025-08-27T02:24:50.786577Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-08-27-02-24-50-812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................\u001b[34m2025-08-27 02:28:55.574072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-08-27 02:28:55.574140: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:28:58.100596: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-08-27 02:28:58.100650: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:28:58.100681: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-130-162.ap-southeast-1.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-08-27 02:28:58.101378: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,008 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:ap-southeast-1:837028399719:processing-job/baseline-suggestion-job-2025-08-27-02-24-50-812', 'ProcessingJobName': 'baseline-suggestion-job-2025-08-27-02-24-50-812', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '245545462676.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://iti113-team2-bucket/Team2/processing/train/v1/train.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://iti113-team2-bucket/Team2/monitoring/baseline', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.t3.large', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::837028399719:role/iti113-team2-sagemaker-iti113-team2-domain-iti113-team2-Role', 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,008 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,008 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,009 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,009 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,009 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,430 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,433 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,433 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.t3.large', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.t3.large', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,448 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,449 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:01,449 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:04,724 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.130.162\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/\u001b[0m\n",
      "\u001b[34mhadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:04,776 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:04,809 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-3be348b8-2e71-4a54-9372-d011882b68d6\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:06,840 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:06,941 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:06,962 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:06,976 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,012 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,012 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,012 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,012 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,138 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,202 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,208 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,215 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,218 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Aug 27 02:29:07\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,221 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,221 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,225 INFO util.GSet: 2.0% max memory 1.4 GB = 28.9 MB\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,226 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,290 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,295 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,296 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,296 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,296 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,296 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,296 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,296 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,296 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,296 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,297 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,297 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,455 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,456 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,456 INFO util.GSet: 1.0% max memory 1.4 GB = 14.5 MB\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,456 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,458 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,459 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,459 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,459 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,474 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,485 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,485 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,486 INFO util.GSet: 0.25% max memory 1.4 GB = 3.6 MB\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,486 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,501 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,502 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,502 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,510 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,510 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,515 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,515 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,515 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 444.7 KB\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,515 INFO util.GSet: capacity      = 2^16 = 65536 entries\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,600 INFO namenode.FSImage: Allocated new BlockPoolId: BP-260708377-10.0.130.162-1756261747578\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,623 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,654 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,938 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 386 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:07,978 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:08,002 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.130.162\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:08,030 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:10,213 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:10,213 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:12,371 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:12,372 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:14,790 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:14,790 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:17,129 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:17,130 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:19,780 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:19,782 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:29,793 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:35,367 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:36,188 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:36,269 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:36,297 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,314 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,357 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,358 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,359 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,363 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,421 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 5744, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,443 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,445 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,619 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,634 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,635 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,636 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:37,637 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,352 INFO util.Utils: Successfully started service 'sparkDriver' on port 44369.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,420 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,508 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,550 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,551 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,610 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,675 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-5ccffb68-407f-4691-b833-8820277cad76\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,709 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,787 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:38,848 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.130.162:44369/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1756261777299\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:39,871 INFO client.RMProxy: Connecting to ResourceManager at /10.0.130.162:8032\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:41,135 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:41,136 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:41,149 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (7834 MB per container)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:41,150 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:41,151 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:41,151 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:41,163 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:41,331 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:45,205 INFO yarn.Client: Uploading resource file:/tmp/spark-4a8fd88a-4605-4357-a64e-553e6c7ea4c4/__spark_libs__4809524009985591202.zip -> hdfs://10.0.130.162/user/root/.sparkStaging/application_1756261758708_0001/__spark_libs__4809524009985591202.zip\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:48,130 INFO yarn.Client: Uploading resource file:/tmp/spark-4a8fd88a-4605-4357-a64e-553e6c7ea4c4/__spark_conf__6161906742016290214.zip -> hdfs://10.0.130.162/user/root/.sparkStaging/application_1756261758708_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:48,271 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:48,271 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:48,271 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:48,271 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:48,271 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:48,367 INFO yarn.Client: Submitting application application_1756261758708_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:48,983 INFO impl.YarnClientImpl: Submitted application application_1756261758708_0001\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:49,991 INFO yarn.Client: Application report for application_1756261758708_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:49,997 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Wed Aug 27 02:29:49 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1756261788711\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1756261758708_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:51,009 INFO yarn.Client: Application report for application_1756261758708_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:52,016 INFO yarn.Client: Application report for application_1756261758708_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:53,021 INFO yarn.Client: Application report for application_1756261758708_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:54,029 INFO yarn.Client: Application report for application_1756261758708_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:55,037 INFO yarn.Client: Application report for application_1756261758708_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:56,044 INFO yarn.Client: Application report for application_1756261758708_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:57,055 INFO yarn.Client: Application report for application_1756261758708_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:58,067 INFO yarn.Client: Application report for application_1756261758708_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,074 INFO yarn.Client: Application report for application_1756261758708_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,075 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.130.162\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1756261788711\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1756261758708_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,079 INFO cluster.YarnClientSchedulerBackend: Application application_1756261758708_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,101 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39687.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,101 INFO netty.NettyBlockTransferService: Server created on 10.0.130.162:39687\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,103 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,116 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.130.162, 39687, None)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,122 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.130.162:39687 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.130.162, 39687, None)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,127 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.130.162, 39687, None)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,129 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.130.162, 39687, None)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,409 INFO util.log: Logging initialized @28463ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-08-27 02:29:59,620 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1756261758708_0001), /proxy/application_1756261758708_0001\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:03,683 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:09,544 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:10,376 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:10,589 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:10,600 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:14,256 INFO datasources.InMemoryFileIndex: It took 184 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:14,951 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:15,549 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.130.162:43308) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:16,189 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:16,203 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.130.162:39687 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:16,211 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:16,248 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:38497 with 2.8 GiB RAM, BlockManagerId(1, algo-1, 38497, None)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:17,608 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:17,623 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:17,629 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 131771\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:17,813 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:17,899 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:17,903 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:17,903 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:17,912 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:17,942 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:18,153 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:18,169 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:18,172 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.130.162:39687 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:18,187 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:18,259 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:18,267 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:21,612 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:22,459 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:38497 (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:24,035 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:38497 (size: 39.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:24,636 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3075 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:24,640 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:24,667 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 6.599 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:24,671 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:24,672 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:24,683 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 6.853865 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:25,011 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.130.162:39687 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:25,012 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:38497 in memory (size: 4.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:31,401 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:31,404 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:31,408 INFO datasources.FileSourceStrategy: Output Data Schema: struct<restingBP: string, serumcholestrol: string, maxheartrate: string, oldpeak: string, gender_0: string ... 24 more fields>\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:31,520 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:31,852 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:31,882 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:31,888 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.130.162:39687 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:31,891 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:31,923 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,011 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,015 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,016 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,016 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,021 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,028 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,143 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 20.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,150 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,151 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.130.162:39687 (size: 8.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,152 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,154 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,154 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,161 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:32,235 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:38497 (size: 8.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:34,064 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:38497 (size: 39.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:34,461 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:38497 (size: 58.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:34,800 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2643 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:34,801 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:34,802 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.769 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:34,803 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:34,804 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:34,804 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.790703 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:35,423 INFO codegen.CodeGenerator: Code generated in 446.209237 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:36,551 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.130.162:39687 in memory (size: 8.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:36,566 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:38497 in memory (size: 8.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,214 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,224 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,226 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,227 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,231 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,237 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,281 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 117.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,287 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,288 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.130.162:39687 (size: 35.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,289 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,297 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,298 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,310 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:37,362 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:38497 (size: 35.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:39,865 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2558 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:39,868 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 2.623 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:39,877 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:39,878 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:39,879 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:39,880 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:39,883 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,073 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,076 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,077 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,077 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,079 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,080 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,114 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 170.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,119 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,120 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.130.162:39687 (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,122 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,123 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,124 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,129 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,168 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:38497 (size: 46.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,241 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,963 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 835 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,964 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,971 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.871 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,975 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,975 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:40,977 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.903601 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,095 INFO codegen.CodeGenerator: Code generated in 92.842405 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,701 INFO codegen.CodeGenerator: Code generated in 77.363151 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,885 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,894 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,897 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,898 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,899 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,910 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,950 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 41.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,972 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,974 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.130.162:39687 (size: 17.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,988 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,996 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:41,996 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:42,011 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:42,062 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:38497 (size: 17.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:42,798 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 789 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:42,799 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.886 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:42,800 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:42,801 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:42,801 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:42,802 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.917086 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,730 INFO codegen.CodeGenerator: Code generated in 132.559114 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,750 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,750 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,750 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,750 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,751 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,753 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,761 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 76.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,764 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,766 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.130.162:39687 (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,768 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,769 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,770 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,772 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:43,794 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:38497 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,040 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 269 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,041 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,042 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.286 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,044 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,044 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,044 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,045 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,372 INFO codegen.CodeGenerator: Code generated in 109.228699 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,387 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,389 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,389 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,389 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,390 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,391 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,399 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,409 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,410 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.130.162:39687 (size: 19.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,411 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,412 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,412 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,414 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,438 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:38497 (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,468 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,700 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 286 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,700 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,701 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.309 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,702 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,702 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,703 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.315874 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:44,881 INFO codegen.CodeGenerator: Code generated in 129.465161 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,034 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,041 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,041 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,042 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,042 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,042 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,045 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,065 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 33.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,068 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,069 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.130.162:39687 (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,070 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,071 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,071 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,073 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:45,102 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:38497 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,830 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 2757 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,831 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,832 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 2.786 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,834 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,835 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,835 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,835 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,836 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,839 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,847 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,848 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.130.162:39687 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,849 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,850 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,851 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,854 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,876 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:38497 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,886 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,952 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 100 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,955 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,958 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.120 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,958 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,959 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:47,960 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 2.924946 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,502 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,503 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,503 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,503 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,504 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,505 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,520 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 86.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,523 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,529 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.130.162:39687 (size: 28.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,531 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,532 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,532 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,544 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,572 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:38497 (size: 28.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,950 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 406 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,951 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,953 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.443 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,953 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,953 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,954 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:48,954 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,035 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,040 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,040 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,040 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,042 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,044 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,059 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 171.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,062 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,064 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.130.162:39687 (size: 47.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,065 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,066 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,066 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,068 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,092 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:38497 (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,137 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,455 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 387 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,456 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,457 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.408 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,458 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,458 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,459 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.423445 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,723 INFO codegen.CodeGenerator: Code generated in 18.092023 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,876 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,880 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,880 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,881 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,888 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,888 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,926 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.130.162:39687 in memory (size: 35.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:49,929 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:38497 in memory (size: 35.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,036 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 41.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,048 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,053 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.130.162:39687 (size: 17.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,056 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,092 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,094 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,095 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.130.162:39687 in memory (size: 17.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,105 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,177 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:38497 (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,199 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:38497 in memory (size: 17.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,284 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.130.162:39687 in memory (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,290 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:38497 in memory (size: 46.9 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,383 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 278 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,384 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,385 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.491 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,386 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,387 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,388 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.508537 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,393 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:38497 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,406 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.130.162:39687 in memory (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,472 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:38497 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,487 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.130.162:39687 in memory (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,517 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:38497 in memory (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,533 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.130.162:39687 in memory (size: 19.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,581 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.130.162:39687 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,589 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:38497 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,617 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.130.162:39687 in memory (size: 28.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,623 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:38497 in memory (size: 28.3 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,665 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:38497 in memory (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,670 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.130.162:39687 in memory (size: 47.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,924 INFO codegen.CodeGenerator: Code generated in 117.138249 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,938 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,939 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,939 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,939 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,941 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,944 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,951 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 76.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,956 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,957 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.130.162:39687 (size: 24.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,960 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,961 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,962 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,964 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:50,987 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:38497 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,170 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 206 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,170 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,172 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.224 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,172 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,172 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,172 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,172 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,344 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,346 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,347 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,347 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,348 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,350 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,356 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,361 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,362 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.130.162:39687 (size: 19.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,364 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,364 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,365 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,368 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,396 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:38497 (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,412 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,437 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 69 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,438 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,439 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.085 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,440 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,441 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,442 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.096479 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,574 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,576 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,577 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,578 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,578 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,578 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,580 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,599 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 33.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,602 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,604 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.130.162:39687 (size: 15.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,608 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,609 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,616 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,618 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,643 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:38497 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,726 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 108 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,728 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,728 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.146 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,729 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,730 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,730 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,730 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,732 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,736 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,739 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,741 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.130.162:39687 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,742 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,743 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,743 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,745 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,762 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:38497 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,769 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,795 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,795 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,796 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,796 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,796 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:51,797 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.222349 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,218 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,219 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,219 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,220 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,221 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,221 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,228 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 86.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,232 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,233 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.130.162:39687 (size: 28.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,234 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,235 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,235 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,237 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,259 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:38497 (size: 28.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,473 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 236 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,473 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,474 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.250 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,476 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,476 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,477 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,477 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,577 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,579 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,579 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,580 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,580 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,581 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,605 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 171.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,617 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,619 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.130.162:39687 (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,619 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,620 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,620 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,623 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,643 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:38497 (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,670 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,922 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 300 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,924 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,926 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.342 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,928 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,928 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:52,929 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.351006 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,119 INFO codegen.CodeGenerator: Code generated in 17.988785 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,158 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,159 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,160 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,160 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,161 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,162 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,170 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 41.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,173 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,174 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.130.162:39687 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,175 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,175 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,176 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,183 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,205 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:38497 (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,350 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 168 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,350 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,352 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.188 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,352 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,352 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,353 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.194514 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,689 INFO codegen.CodeGenerator: Code generated in 104.82231 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,697 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,698 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,698 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,699 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,699 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,700 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,715 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 76.5 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,718 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,719 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.130.162:39687 (size: 24.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,719 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,722 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,723 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,725 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,744 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:38497 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,978 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 254 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,979 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,980 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.278 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,981 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,981 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,981 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:53,981 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,118 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,124 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,125 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,125 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,126 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,126 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,131 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,136 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,137 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.130.162:39687 (size: 19.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,138 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,138 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,141 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,143 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,163 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:38497 (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,170 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,180 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,180 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,181 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,182 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,183 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,183 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.059954 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,307 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,312 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,313 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,314 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,314 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,314 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,322 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,329 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 33.3 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,331 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,338 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.130.162:39687 (size: 15.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,339 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,340 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,340 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,342 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,357 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:38497 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,454 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 113 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,454 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,455 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.131 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,456 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,456 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,456 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,456 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,457 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,459 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,461 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,462 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.130.162:39687 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,463 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,469 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,470 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,471 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,487 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:38497 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,493 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,519 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 48 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,520 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,520 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,521 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,521 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,523 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.211663 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,773 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,774 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,775 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,775 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,776 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,777 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,787 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 86.2 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,797 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 28.1 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,801 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.130.162:39687 (size: 28.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,804 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,804 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,805 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,808 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:54,827 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:38497 (size: 28.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,008 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 201 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,008 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,009 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.231 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,010 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,011 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,011 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,011 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,106 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,110 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,110 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,110 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,111 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,111 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,135 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 171.1 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,141 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,145 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.130.162:39687 (size: 47.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,147 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,147 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,148 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,150 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,167 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:38497 (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,191 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,354 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 205 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,355 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,355 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.237 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,362 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,362 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,363 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.256411 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,640 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,641 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,642 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,642 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,643 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,644 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,665 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 41.0 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,676 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,677 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.130.162:39687 (size: 17.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,678 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,679 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,679 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,681 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,703 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:38497 (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,751 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 70 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,753 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.106 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,753 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,757 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,758 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:55,758 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.118217 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,049 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.130.162:39687 in memory (size: 17.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,053 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,054 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,054 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,054 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,060 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:38497 in memory (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,064 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,067 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,078 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 76.5 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,080 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,088 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:38497 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,090 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.130.162:39687 (size: 24.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,091 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,092 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,092 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,094 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,099 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.130.162:39687 in memory (size: 24.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,143 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.130.162:39687 in memory (size: 28.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,149 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:38497 in memory (size: 28.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,154 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:38497 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,211 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,211 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,212 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.144 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,214 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,214 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,215 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,215 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,227 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:38497 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,242 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.130.162:39687 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,306 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.130.162:39687 in memory (size: 15.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,320 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,322 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,324 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,324 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,323 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:38497 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,325 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,325 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,330 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 66.2 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,332 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,333 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.130.162:39687 (size: 19.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,334 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,335 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,335 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,337 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,368 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:38497 (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,376 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,400 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 64 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,401 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,401 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.073 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,402 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,402 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,402 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.081725 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,445 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.130.162:39687 in memory (size: 47.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,450 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:38497 in memory (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,489 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,490 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,490 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,491 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,491 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,491 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,493 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,506 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.130.162:39687 in memory (size: 17.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,514 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:38497 in memory (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,529 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 33.3 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,535 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,536 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.130.162:39687 (size: 15.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,539 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,540 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,541 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,542 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,565 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:38497 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,586 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:38497 in memory (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,596 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.130.162:39687 in memory (size: 17.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,648 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:38497 in memory (size: 28.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,656 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.130.162:39687 in memory (size: 28.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,666 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:38497 in memory (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,677 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.130.162:39687 in memory (size: 19.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,711 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:38497 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,713 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.130.162:39687 in memory (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,737 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 195 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,738 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.244 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,739 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,739 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,739 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,740 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,740 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,739 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,746 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,748 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,750 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.130.162:39687 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,751 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,752 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,752 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,759 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,773 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:38497 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,774 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.130.162:39687 in memory (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,802 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:38497 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,807 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:38497 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,811 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,819 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.130.162:39687 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,831 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.130.162:39687 in memory (size: 19.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,832 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:38497 in memory (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,836 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,837 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.093 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,838 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,839 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,839 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,840 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.351108 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,842 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.130.162:39687 in memory (size: 47.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:56,912 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:38497 in memory (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,206 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,207 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,207 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,207 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,208 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,209 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,214 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 86.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,217 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,217 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.130.162:39687 (size: 28.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,218 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,219 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,219 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,221 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,240 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:38497 (size: 28.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,430 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 209 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,431 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,432 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.222 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,433 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,435 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,435 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,435 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,486 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,488 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,488 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,488 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,489 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,489 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,498 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 171.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,501 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,502 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.130.162:39687 (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,503 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,504 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,504 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,506 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,524 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:38497 (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,553 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,682 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 177 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,682 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,684 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.193 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,684 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,685 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,685 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.198424 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,957 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,958 INFO scheduler.DAGScheduler: Got job 28 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,959 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,959 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,960 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,961 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,968 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 41.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,978 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,982 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.130.162:39687 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,983 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,983 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,984 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:57,985 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,002 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:38497 (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,031 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 46 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,037 INFO scheduler.DAGScheduler: ResultStage 41 (treeReduce at KLLRunner.scala:107) finished in 0.075 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,038 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,038 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,039 INFO cluster.YarnScheduler: Killing all running tasks in stage 41: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,039 INFO scheduler.DAGScheduler: Job 28 finished: treeReduce at KLLRunner.scala:107, took 0.081418 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,289 INFO scheduler.DAGScheduler: Registering RDD 174 (collect at AnalysisRunner.scala:326) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,290 INFO scheduler.DAGScheduler: Got map stage job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,290 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,290 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,291 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,291 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,296 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 76.5 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,301 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 24.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,302 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.130.162:39687 (size: 24.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,303 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,304 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,304 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,306 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,324 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:38497 (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,358 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 52 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,359 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,360 INFO scheduler.DAGScheduler: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326) finished in 0.067 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,360 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,360 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,360 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,361 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,443 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,444 INFO scheduler.DAGScheduler: Got job 30 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,451 INFO scheduler.DAGScheduler: Final stage: ResultStage 44 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,451 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,452 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,452 INFO scheduler.DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,455 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 66.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,459 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,459 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.130.162:39687 (size: 19.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,461 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,461 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,462 INFO cluster.YarnScheduler: Adding task set 44.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,464 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,504 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:38497 (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,514 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,532 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 34) in 69 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,533 INFO cluster.YarnScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,534 INFO scheduler.DAGScheduler: ResultStage 44 (collect at AnalysisRunner.scala:326) finished in 0.080 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,535 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,535 INFO cluster.YarnScheduler: Killing all running tasks in stage 44: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,536 INFO scheduler.DAGScheduler: Job 30 finished: collect at AnalysisRunner.scala:326, took 0.093274 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,626 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,627 INFO scheduler.DAGScheduler: Registering RDD 185 (countByKey at ColumnProfiler.scala:592) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,628 INFO scheduler.DAGScheduler: Got job 31 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,628 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,628 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,628 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,629 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,637 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 33.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,639 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,640 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.130.162:39687 (size: 15.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,640 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,641 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,641 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,643 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,659 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:38497 (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,727 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 84 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,728 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,729 INFO scheduler.DAGScheduler: ShuffleMapStage 45 (countByKey at ColumnProfiler.scala:592) finished in 0.099 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,729 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,733 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,734 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 46)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,734 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,735 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,737 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 5.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,739 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,740 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.130.162:39687 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,741 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,742 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,742 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,744 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 36) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,758 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:38497 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,764 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,786 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 36) in 42 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,786 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,787 INFO scheduler.DAGScheduler: ResultStage 46 (countByKey at ColumnProfiler.scala:592) finished in 0.051 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,788 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,788 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,789 INFO scheduler.DAGScheduler: Job 31 finished: countByKey at ColumnProfiler.scala:592, took 0.162286 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,850 INFO scheduler.DAGScheduler: Registering RDD 191 (collect at AnalysisRunner.scala:326) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,851 INFO scheduler.DAGScheduler: Got map stage job 32 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,851 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,851 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,852 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,853 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,857 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 45.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,859 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,860 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.130.162:39687 (size: 18.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,861 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,862 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,862 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,866 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 37) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:58,893 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:38497 (size: 18.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,161 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 37) in 296 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,161 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,162 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326) finished in 0.307 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,164 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,164 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,164 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,164 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,215 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,216 INFO scheduler.DAGScheduler: Got job 33 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,217 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,217 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,217 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,218 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,222 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 65.3 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,225 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,226 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.130.162:39687 (size: 24.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,227 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,227 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,228 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,229 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 38) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,245 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:38497 (size: 24.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,255 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,393 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 38) in 164 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,396 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,398 INFO scheduler.DAGScheduler: ResultStage 49 (collect at AnalysisRunner.scala:326) finished in 0.179 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,399 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,399 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,400 INFO scheduler.DAGScheduler: Job 33 finished: collect at AnalysisRunner.scala:326, took 0.184570 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,418 INFO codegen.CodeGenerator: Code generated in 15.568657 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,492 INFO codegen.CodeGenerator: Code generated in 21.521228 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,560 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,561 INFO scheduler.DAGScheduler: Got job 34 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,561 INFO scheduler.DAGScheduler: Final stage: ResultStage 50 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,562 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,562 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,563 INFO scheduler.DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,581 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 36.4 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,583 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,584 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.130.162:39687 (size: 16.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,584 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,589 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,589 INFO cluster.YarnScheduler: Adding task set 50.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,591 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 39) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,606 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-1:38497 (size: 16.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,696 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 39) in 105 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,697 INFO scheduler.DAGScheduler: ResultStage 50 (treeReduce at KLLRunner.scala:107) finished in 0.133 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,698 INFO scheduler.DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,699 INFO cluster.YarnScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,699 INFO cluster.YarnScheduler: Killing all running tasks in stage 50: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,699 INFO scheduler.DAGScheduler: Job 34 finished: treeReduce at KLLRunner.scala:107, took 0.139176 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,830 INFO codegen.CodeGenerator: Code generated in 33.550687 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,837 INFO scheduler.DAGScheduler: Registering RDD 209 (collect at AnalysisRunner.scala:326) as input to shuffle 16\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,837 INFO scheduler.DAGScheduler: Got map stage job 35 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,837 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,838 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,838 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,839 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,846 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 35.8 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,849 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 14.5 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,851 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.130.162:39687 (size: 14.5 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,852 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,852 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,853 INFO cluster.YarnScheduler: Adding task set 51.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,854 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 40) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,874 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-1:38497 (size: 14.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,940 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 40) in 86 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,941 INFO cluster.YarnScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,941 INFO scheduler.DAGScheduler: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326) finished in 0.098 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,942 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,942 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,942 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:30:59,942 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,054 INFO codegen.CodeGenerator: Code generated in 55.769687 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,077 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,078 INFO scheduler.DAGScheduler: Got job 36 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,079 INFO scheduler.DAGScheduler: Final stage: ResultStage 53 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,079 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,079 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,080 INFO scheduler.DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,082 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 21.4 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,084 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,084 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.130.162:39687 (size: 8.5 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,085 INFO spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,094 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,094 INFO cluster.YarnScheduler: Adding task set 53.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,096 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 41) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,107 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-1:38497 (size: 8.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,113 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,182 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 41) in 86 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,183 INFO cluster.YarnScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,184 INFO scheduler.DAGScheduler: ResultStage 53 (collect at AnalysisRunner.scala:326) finished in 0.103 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,184 INFO scheduler.DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,185 INFO cluster.YarnScheduler: Killing all running tasks in stage 53: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,185 INFO scheduler.DAGScheduler: Job 36 finished: collect at AnalysisRunner.scala:326, took 0.107735 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,219 INFO codegen.CodeGenerator: Code generated in 26.694749 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,313 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,315 INFO scheduler.DAGScheduler: Registering RDD 220 (countByKey at ColumnProfiler.scala:592) as input to shuffle 17\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,315 INFO scheduler.DAGScheduler: Got job 37 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,316 INFO scheduler.DAGScheduler: Final stage: ResultStage 55 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,316 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,316 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,320 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,328 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.9 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,330 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,332 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.130.162:39687 (size: 14.8 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,333 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,334 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,341 INFO cluster.YarnScheduler: Adding task set 54.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,344 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 42) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,356 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-1:38497 (size: 14.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,442 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 42) in 98 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,442 INFO cluster.YarnScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,443 INFO scheduler.DAGScheduler: ShuffleMapStage 54 (countByKey at ColumnProfiler.scala:592) finished in 0.122 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,444 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,444 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,444 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 55)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,445 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,445 INFO scheduler.DAGScheduler: Submitting ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,447 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 5.1 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,452 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,454 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.130.162:39687 (size: 3.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,454 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,456 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,457 INFO cluster.YarnScheduler: Adding task set 55.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,468 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 55.0 (TID 43) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,483 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-1:38497 (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,488 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,512 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 55.0 (TID 43) in 45 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,513 INFO scheduler.DAGScheduler: ResultStage 55 (countByKey at ColumnProfiler.scala:592) finished in 0.067 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,514 INFO scheduler.DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,515 INFO cluster.YarnScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,515 INFO cluster.YarnScheduler: Killing all running tasks in stage 55: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:00,516 INFO scheduler.DAGScheduler: Job 37 finished: countByKey at ColumnProfiler.scala:592, took 0.201597 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,039 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,187 INFO codegen.CodeGenerator: Code generated in 72.50511 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,216 INFO scheduler.DAGScheduler: Registering RDD 226 (count at StatsGenerator.scala:66) as input to shuffle 18\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,217 INFO scheduler.DAGScheduler: Got map stage job 38 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,222 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 56 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,225 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,226 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,227 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[226] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,246 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 25.4 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,247 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,248 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.130.162:39687 (size: 11.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,249 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,250 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[226] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,250 INFO cluster.YarnScheduler: Adding task set 56.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,256 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 56.0 (TID 44) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,290 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-1:38497 (size: 11.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,422 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 56.0 (TID 44) in 166 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,423 INFO cluster.YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,423 INFO scheduler.DAGScheduler: ShuffleMapStage 56 (count at StatsGenerator.scala:66) finished in 0.195 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,423 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,423 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,423 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,423 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,558 INFO codegen.CodeGenerator: Code generated in 33.86827 ms\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,588 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,589 INFO scheduler.DAGScheduler: Got job 39 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,589 INFO scheduler.DAGScheduler: Final stage: ResultStage 58 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,589 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,589 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,598 INFO scheduler.DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[229] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,601 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 11.1 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,643 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,649 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.130.162:39687 (size: 5.5 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,662 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,663 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[229] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,668 INFO cluster.YarnScheduler: Adding task set 58.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,669 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:38497 in memory (size: 28.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,670 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 58.0 (TID 45) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,675 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.130.162:39687 in memory (size: 28.2 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,680 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.0.130.162:39687 in memory (size: 3.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,691 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on algo-1:38497 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,707 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on algo-1:38497 (size: 5.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,712 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.0.130.162:39687 in memory (size: 16.2 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,716 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-1:38497 in memory (size: 16.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,717 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.130.162:43308\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,738 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:38497 in memory (size: 17.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,746 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.130.162:39687 in memory (size: 17.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,749 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.130.162:39687 in memory (size: 15.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,752 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:38497 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,764 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.0.130.162:39687 in memory (size: 24.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,767 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:38497 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,780 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.130.162:39687 in memory (size: 47.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,787 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:38497 in memory (size: 47.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,793 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.0.130.162:39687 in memory (size: 8.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,795 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on algo-1:38497 in memory (size: 8.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,805 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.0.130.162:39687 in memory (size: 11.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,809 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 58.0 (TID 45) in 139 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,809 INFO cluster.YarnScheduler: Removed TaskSet 58.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,810 INFO scheduler.DAGScheduler: ResultStage 58 (count at StatsGenerator.scala:66) finished in 0.211 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,812 INFO scheduler.DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,812 INFO cluster.YarnScheduler: Killing all running tasks in stage 58: Stage finished\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,813 INFO scheduler.DAGScheduler: Job 39 finished: count at StatsGenerator.scala:66, took 0.224667 s\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,822 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on algo-1:38497 in memory (size: 11.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,840 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.0.130.162:39687 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,845 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-1:38497 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,904 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.130.162:39687 in memory (size: 19.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,907 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:38497 in memory (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,923 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.0.130.162:39687 in memory (size: 15.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,925 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-1:38497 in memory (size: 15.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,937 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on algo-1:38497 in memory (size: 14.8 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,939 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.0.130.162:39687 in memory (size: 14.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,945 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.0.130.162:39687 in memory (size: 24.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,947 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-1:38497 in memory (size: 24.2 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,953 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.0.130.162:39687 in memory (size: 18.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,954 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on algo-1:38497 in memory (size: 18.6 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,970 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.130.162:39687 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,974 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:38497 in memory (size: 3.0 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,992 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.0.130.162:39687 in memory (size: 14.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:01,994 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on algo-1:38497 in memory (size: 14.5 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,014 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.0.130.162:39687 in memory (size: 19.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,019 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:38497 in memory (size: 19.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,029 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.130.162:39687 in memory (size: 24.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,034 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:38497 in memory (size: 24.1 KiB, free: 2.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,641 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,676 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,796 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,800 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,814 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,867 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,948 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,953 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,972 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:02,982 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:03,066 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:03,066 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:03,066 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:03,089 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:03,091 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-2819fa2b-4da3-4203-971d-861e5b7e173e\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:03,112 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-4a8fd88a-4605-4357-a64e-553e6c7ea4c4\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:03,337 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-08-27 02:31:03,338 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DefaultModelMonitor\n",
    "\n",
    "bucket_name = 'iti113-team2-bucket'\n",
    "base_folder = 'Team2'\n",
    "\n",
    "monitor_output_path = f\"s3://{bucket_name}/{base_folder}/monitoring\"\n",
    "baseline_data = f's3://{bucket_name}/{base_folder}/processing/train/v1/train.csv'\n",
    "\n",
    "# Create a DefaultModelMonitor instance\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.large',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Run the baseline job\n",
    "baseline_job = monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=f\"{monitor_output_path}/baseline\",\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde279f-11a0-4802-b0d6-3a3c86ccefdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8f828e0-b418-4094-afa1-c83956139fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T02:45:17.766898Z",
     "iopub.status.busy": "2025-08-27T02:45:17.766254Z",
     "iopub.status.idle": "2025-08-27T02:45:18.687520Z",
     "shell.execute_reply": "2025-08-27T02:45:18.686836Z",
     "shell.execute_reply.started": "2025-08-27T02:45:17.766866Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: Team2-monitor-schedule\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'iti113-team2-bucket'\n",
    "base_folder = 'Team2'\n",
    "\n",
    "monitor_output_path = f\"s3://{bucket_name}/{base_folder}/monitoring\"\n",
    "\n",
    "endpoint_name = \"Team2-predictor-endpoint\"\n",
    "schedule_name = 'Team2-monitor-schedule'\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=schedule_name,\n",
    "    endpoint_input=endpoint_name,\n",
    "    output_s3_uri=f\"{monitor_output_path}/output\",\n",
    "    statistics=f\"{monitor_output_path}/baseline/statistics.json\",\n",
    "    constraints=f\"{monitor_output_path}/baseline/constraints.json\",\n",
    "    schedule_cron_expression='cron(0 * ? * * *)'  # hourly\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b287189-d5de-408a-82a8-68950d7c471a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T02:45:39.002556Z",
     "iopub.status.busy": "2025-08-27T02:45:39.002164Z",
     "iopub.status.idle": "2025-08-27T02:45:39.108889Z",
     "shell.execute_reply": "2025-08-27T02:45:39.108044Z",
     "shell.execute_reply.started": "2025-08-27T02:45:39.002530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '71157dc6-6ff2-4b25-929b-b4fd9ddb7c1a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '71157dc6-6ff2-4b25-929b-b4fd9ddb7c1a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Wed, 27 Aug 2025 02:45:39 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "schedule_name = 'Team2-monitor-schedule'\n",
    "sm = boto3.client('sagemaker')\n",
    "sm.start_monitoring_schedule(MonitoringScheduleName=schedule_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a674248-3c8c-4ee5-94ef-2fc6f926001e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T02:45:46.640978Z",
     "iopub.status.busy": "2025-08-27T02:45:46.639978Z",
     "iopub.status.idle": "2025-08-27T02:45:46.723524Z",
     "shell.execute_reply": "2025-08-27T02:45:46.722842Z",
     "shell.execute_reply.started": "2025-08-27T02:45:46.640870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Scheduled\n",
      "Last execution: None\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "schedule_name = 'Team2-monitor-schedule'\n",
    "response = sm.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "print(\"Status:\", response['MonitoringScheduleStatus'])\n",
    "\n",
    "last_run = response.get('LastMonitoringExecutionSummary', {})\n",
    "print(\"Last execution:\", last_run.get('MonitoringExecutionStatus'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c082239a-c024-4cf1-a40f-e874eae37686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T02:46:03.106779Z",
     "iopub.status.busy": "2025-08-27T02:46:03.106202Z",
     "iopub.status.idle": "2025-08-27T02:46:03.209486Z",
     "shell.execute_reply": "2025-08-27T02:46:03.208723Z",
     "shell.execute_reply.started": "2025-08-27T02:46:03.106754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'}, 'MonitoringJobDefinitionName': 'data-quality-job-definition-2025-08-27-02-45-17-768', 'MonitoringType': 'DataQuality'}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "schedule_name = 'Team2-monitor-schedule'\n",
    "response = client.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "print(response['MonitoringScheduleConfig'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5ca948e-f3bb-48a8-93f7-2cb88d5b03ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T02:46:55.083462Z",
     "iopub.status.busy": "2025-08-27T02:46:55.083116Z",
     "iopub.status.idle": "2025-08-27T02:46:55.174179Z",
     "shell.execute_reply": "2025-08-27T02:46:55.173431Z",
     "shell.execute_reply.started": "2025-08-27T02:46:55.083438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring schedule 'Team2-monitor-schedule' deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "#delete monitor\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "schedule_name = 'Team2-monitor-schedule'\n",
    "\n",
    "# Delete the monitoring schedule\n",
    "response = sm.delete_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "\n",
    "print(f\"Monitoring schedule '{schedule_name}' deleted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48dfd0-4694-4556-9e6b-06c2c25ea355",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "------\n",
    "#### Re-attach to the job as a ProcessingJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a75536c2-d6a7-45ed-9cd9-ad1cf060db9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T15:21:32.239768Z",
     "iopub.status.busy": "2025-08-25T15:21:32.239370Z",
     "iopub.status.idle": "2025-08-25T15:21:32.465093Z",
     "shell.execute_reply": "2025-08-25T15:21:32.464331Z",
     "shell.execute_reply.started": "2025-08-25T15:21:32.239742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully re-attached to job: baseline-suggestion-job-2025-08-25-14-21-55-214\n",
      "\n",
      "Loading constraints from: s3://iti113-team2-bucket/Team2/monitoring/baseline/constraints.json\n",
      "\n",
      "--- Sample of Generated Constraints ---\n",
      "{'features': [{'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'restingBP',\n",
      "               'num_constraints': {'is_non_negative': False}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'serumcholestrol',\n",
      "               'num_constraints': {'is_non_negative': False}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'maxheartrate',\n",
      "               'num_constraints': {'is_non_negative': False}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'oldpeak',\n",
      "               'num_constraints': {'is_non_negative': False}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'gender_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'gender_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'chestpain_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'chestpain_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'chestpain_2',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'chestpain_3',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'fastingbloodsugar_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'fastingbloodsugar_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'restingrelectro_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'restingrelectro_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'restingrelectro_2',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'exerciseangia_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'exerciseangia_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'slope_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'slope_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'slope_2',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'slope_3',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'noofmajorvessels_0',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'noofmajorvessels_1',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'noofmajorvessels_2',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Fractional',\n",
      "               'name': 'noofmajorvessels_3',\n",
      "               'num_constraints': {'is_non_negative': True}},\n",
      "              {'completeness': 1.0,\n",
      "               'inferred_type': 'Integral',\n",
      "               'name': 'target',\n",
      "               'num_constraints': {'is_non_negative': True}}],\n",
      " 'monitoring_config': {'datatype_check_threshold': 1.0,\n",
      "                       'distribution_constraints': {'categorical_comparison_threshold': 0.1,\n",
      "                                                    'categorical_drift_method': 'LInfinity',\n",
      "                                                    'comparison_method': 'Robust',\n",
      "                                                    'comparison_threshold': 0.1,\n",
      "                                                    'perform_comparison': 'Enabled'},\n",
      "                       'domain_content_threshold': 1.0,\n",
      "                       'emit_metrics': 'Enabled',\n",
      "                       'evaluate_constraints': 'Enabled'},\n",
      " 'version': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingJob\n",
    "from sagemaker.model_monitor import Constraints\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "baseline_job = None\n",
    "# !!!! The job name from your successful run\n",
    "baseline_job_name = \"baseline-suggestion-job-2025-08-25-14-21-55-214\"\n",
    "\n",
    "try:\n",
    "    # Re-attach as a ProcessingJob\n",
    "    baseline_job = ProcessingJob.from_processing_name(\n",
    "        processing_job_name=baseline_job_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    print(f\"Successfully re-attached to job: {baseline_job_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to re-attach to the job. Error: {e}\")\n",
    "\n",
    "\n",
    "# Now, load and inspect the constraints from S3\n",
    "if baseline_job:\n",
    "    # Find the output configuration in the job description\n",
    "    output_config = baseline_job.describe()['ProcessingOutputConfig']['Outputs']\n",
    "\n",
    "    # Find the S3 URI for the baseline output\n",
    "    baseline_output_uri = None\n",
    "    for output in output_config:\n",
    "        if output['OutputName'] == 'monitoring_output':\n",
    "            baseline_output_uri = output['S3Output']['S3Uri']\n",
    "            break\n",
    "\n",
    "    if baseline_output_uri:\n",
    "        # Construct the full path to the constraints file\n",
    "        constraints_s3_uri = f\"{baseline_output_uri}/constraints.json\"\n",
    "        print(f\"\\nLoading constraints from: {constraints_s3_uri}\")\n",
    "\n",
    "        # Load the constraints file from S3\n",
    "        suggested_constraints = Constraints.from_s3_uri(constraints_s3_uri)\n",
    "\n",
    "        # Now print the dictionary, just like in the other example\n",
    "        print(\"\\n--- Sample of Generated Constraints ---\")\n",
    "        from pprint import pprint\n",
    "        pprint(suggested_constraints.body_dict)\n",
    "        # ===================================================================\n",
    "    else:\n",
    "        print(\"Could not find the baseline output path in the job description.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e525b11-c6ad-4e2a-b1b2-9464a9ec35c1",
   "metadata": {},
   "source": [
    "----\n",
    "#### Schedule the Monitoring Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "197b14dc-abde-446b-9711-fc4c3e3178ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:20:51.749534Z",
     "iopub.status.busy": "2025-08-27T05:20:51.749239Z",
     "iopub.status.idle": "2025-08-27T05:20:52.880370Z",
     "shell.execute_reply": "2025-08-27T05:20:52.879519Z",
     "shell.execute_reply.started": "2025-08-27T05:20:51.749513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint status: InService\n",
      "SageMaker default region: ap-southeast-1\n",
      "No schedule named 'Team2-drift-schedule-main' found. Creating a new one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: Team2-drift-schedule-main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring schedule 'Team2-drift-schedule-main' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator, DefaultModelMonitor\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\", region_name=\"ap-southeast-1\")\n",
    "\n",
    "# Confirm endpoint is InService\n",
    "endpoint_name = \"Team2-predictor-endpoint\"\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(\"Endpoint status:\", response['EndpointStatus'])  # should be \"InService\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "print(\"SageMaker default region:\", sagemaker_session.boto_region_name)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket_name = 'iti113-team2-bucket'\n",
    "base_folder = 'Team2'\n",
    "\n",
    "monitor_output_path = f\"s3://{bucket_name}/{base_folder}/monitoring\"\n",
    "baseline_output_uri = f\"{monitor_output_path}/baseline\"\n",
    "\n",
    "#schedule_name\n",
    "schedule_name = \"Team2-drift-schedule-main\"\n",
    "\n",
    "# Initialize model monitor\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.large',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Check if schedule exists\n",
    "    sagemaker_client.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "    print(f\"Found existing monitoring schedule: '{schedule_name}'\")\n",
    "\n",
    "    # Attach monitor to the schedule\n",
    "    monitor.attach(schedule_name)\n",
    "    monitor.monitoring_schedule_name = schedule_name\n",
    "    print(f\"Successfully attached monitor object to the schedule.\")\n",
    "\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceNotFound':\n",
    "        print(f\"No schedule named '{schedule_name}' found. Creating a new one.\")\n",
    "\n",
    "        # endpoint_input = EndpointInput(\n",
    "        #     endpoint_name=\"Team2-predictor-endpoint\",\n",
    "        #     destination=\"/opt/ml/processing/input\"\n",
    "        # )\n",
    "        \n",
    "        # Create monitoring schedule\n",
    "        monitor.create_monitoring_schedule(\n",
    "            monitor_schedule_name=schedule_name,\n",
    "            endpoint_input=endpoint_name,\n",
    "            output_s3_uri=f\"{monitor_output_path}/reports\",\n",
    "            statistics=f\"{baseline_output_uri}/statistics.json\",\n",
    "            constraints=f\"{baseline_output_uri}/constraints.json\",\n",
    "            schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "            enable_cloudwatch_metrics=True,\n",
    "        )\n",
    "        print(f\"Monitoring schedule '{schedule_name}' created successfully.\")\n",
    "        \n",
    "        # Attach after creation\n",
    "        # monitor.attach(monitoring_schedule_name=schedule_name)\n",
    "        \n",
    "    else:\n",
    "        print(\"An unexpected error occurred while checking for the schedule.\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d2fb52bf-57a6-473e-9372-886fa38d5177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:26:04.567466Z",
     "iopub.status.busy": "2025-08-27T05:26:04.567173Z",
     "iopub.status.idle": "2025-08-27T05:26:04.664965Z",
     "shell.execute_reply": "2025-08-27T05:26:04.664192Z",
     "shell.execute_reply.started": "2025-08-27T05:26:04.567444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Scheduled\n"
     ]
    }
   ],
   "source": [
    "# Now describe schedule details\n",
    "try:\n",
    "    schedule_details = monitor.describe_schedule()\n",
    "    print(f\"Schedule status: {schedule_details['MonitoringScheduleStatus']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve schedule details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc4737-f839-4def-9ff6-846247a63d39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "-----\n",
    "\n",
    "#### Simulate, Detect, and Analyze Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "42f394ba-7241-4c60-8333-ea95660f95e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:16:19.683958Z",
     "iopub.status.busy": "2025-08-27T05:16:19.683658Z",
     "iopub.status.idle": "2025-08-27T05:16:19.971686Z",
     "shell.execute_reply": "2025-08-27T05:16:19.971047Z",
     "shell.execute_reply.started": "2025-08-27T05:16:19.683935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original average restingBP: 0\n",
      "New average restingBP: 1.0\n",
      "Original average serumcholestrol: 0\n",
      "New average serumcholestrol: 1.0\n",
      "\n",
      "Number of features: 25\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "bucket_name = 'iti113-team2-bucket'\n",
    "base_folder = 'Team2'\n",
    "endpoint_name = \"Team2-predictor-endpoint\"\n",
    "aws_region = \"ap-southeast-1\"\n",
    "\n",
    "# === SETUP CLIENTS ===\n",
    "boto_session = boto3.Session(region_name=aws_region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "sagemaker_client = boto3.client(\"sagemaker\", region_name=aws_region)\n",
    "s3_client = boto3.client('s3', region_name=aws_region)\n",
    "\n",
    "s3_process_test_path = f\"s3://{bucket_name}/{base_folder}/processing/test/v1/test.csv\"\n",
    "df = pd.read_csv(s3_process_test_path)\n",
    "df = df.drop(\"target\", axis=1)\n",
    "\n",
    "# Select a few rows to manipulate (simulate drift)\n",
    "# drifted_data = df.head(5).copy()\n",
    "drifted_data = df.head(100).copy()\n",
    "\n",
    "# Manually introduce DRIFT\n",
    "print(\"Original average restingBP:\", int(drifted_data['restingBP'].mean()))\n",
    "drifted_data['restingBP'] = 1\n",
    "print(\"New average restingBP:\", drifted_data['restingBP'].mean())\n",
    "\n",
    "print(\"Original average serumcholestrol:\", int(drifted_data['serumcholestrol'].mean()))\n",
    "drifted_data['serumcholestrol'] = 1\n",
    "print(\"New average serumcholestrol:\", drifted_data['serumcholestrol'].mean())\n",
    "\n",
    "# # Print sample payloads\n",
    "# print(\"\\nSample drifted payloads:\")\n",
    "# print(drifted_data.head(2))\n",
    "\n",
    "print(\"\\nNumber of features:\", drifted_data.shape[1])\n",
    "\n",
    "# # Create SageMaker predictor\n",
    "# predictor = sagemaker.predictor.Predictor(\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     serializer=JSONSerializer(),\n",
    "#     deserializer=JSONDeserializer(),\n",
    "#     sagemaker_session=sagemaker_session\n",
    "# )\n",
    "\n",
    "# # === SEND PREDICTION PAYLOADS ===\n",
    "# print(f\"\\nSending {len(drifted_data)} drifted requests to endpoint: {endpoint_name}\")\n",
    "# for i, row in drifted_data.iterrows():\n",
    "#     payload = {\"data\": [row.to_dict()]}\n",
    "#     try:\n",
    "#         response = predictor.predict(payload)\n",
    "#         print(f\"[{i}] ✅ Response: {response}\")\n",
    "#         time.sleep(0.1)  # Optional delay to avoid throttling\n",
    "#     except Exception as e:\n",
    "#         print(f\"[{i}] ❌ Error sending request: {e}\")\n",
    "\n",
    "# print(\"\\n✅ All drifted requests sent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4ffb573f-ba9d-4d3b-b2c6-d9d9af2bc0ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:16:26.701819Z",
     "iopub.status.busy": "2025-08-27T05:16:26.701130Z",
     "iopub.status.idle": "2025-08-27T05:16:26.795729Z",
     "shell.execute_reply": "2025-08-27T05:16:26.793248Z",
     "shell.execute_reply.started": "2025-08-27T05:16:26.701793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ContentType: application/json\n",
      "{\"predictions\": [1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "#test invoke endpoint\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\", region_name=aws_region)\n",
    "\n",
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps({\"data\": drifted_data.to_dict(orient=\"records\")})\n",
    ")\n",
    "print(f\"\\nContentType: {response[\"ContentType\"]}\")\n",
    "print(response[\"Body\"].read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4841fb-e4ba-414e-b9c4-b414394e1fe4",
   "metadata": {},
   "source": [
    "----\n",
    "#### Manual start monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a95aa035-3c1a-4d98-8a20-7f36c63f063f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:26:31.370055Z",
     "iopub.status.busy": "2025-08-27T05:26:31.369309Z",
     "iopub.status.idle": "2025-08-27T05:26:36.539139Z",
     "shell.execute_reply": "2025-08-27T05:26:36.538334Z",
     "shell.execute_reply.started": "2025-08-27T05:26:31.370024Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Starting Monitoring Schedule with name: Team2-drift-schedule-main\n"
     ]
    }
   ],
   "source": [
    "monitor.start_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c67e4311-f43e-41ce-b2f8-370c2baaad39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:38:24.407458Z",
     "iopub.status.busy": "2025-08-27T05:38:24.407185Z",
     "iopub.status.idle": "2025-08-27T05:38:24.459362Z",
     "shell.execute_reply": "2025-08-27T05:38:24.458690Z",
     "shell.execute_reply.started": "2025-08-27T05:38:24.407438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Team2-drift-schedule-main', 'ADL-drift-schedule-main']\n"
     ]
    }
   ],
   "source": [
    "schedules = client.list_monitoring_schedules()[\"MonitoringScheduleSummaries\"]\n",
    "print([s[\"MonitoringScheduleName\"] for s in schedules])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6ad2d-0d26-4b1d-91d5-46e50330fa89",
   "metadata": {},
   "source": [
    "----\n",
    "#### Check scheduled monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cae3b3dd-a5b6-4c2d-b9b3-3a2e1bcc6bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:31:46.943733Z",
     "iopub.status.busy": "2025-08-27T05:31:46.943036Z",
     "iopub.status.idle": "2025-08-27T05:31:47.000923Z",
     "shell.execute_reply": "2025-08-27T05:31:47.000144Z",
     "shell.execute_reply.started": "2025-08-27T05:31:46.943705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'MonitoringScheduleName': 'Team2-drift-schedule-main', 'ScheduledTime': datetime.datetime(2025, 8, 27, 3, 0, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2025, 8, 27, 3, 8, 20, 489000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2025, 8, 27, 3, 15, 58, 571000, tzinfo=tzlocal()), 'MonitoringExecutionStatus': 'Failed', 'ProcessingJobArn': 'arn:aws:sagemaker:ap-southeast-1:837028399719:processing-job/model-monitoring-202508270300-224495b8a80ed9505b8b78d0', 'EndpointName': 'Team2-predictor-endpoint', 'FailureReason': 'AlgorithmError: Error: Errors occurred when analyzing your data. Please check CloudWatch logs for more details., exit code: 255', 'MonitoringJobDefinitionName': 'data-quality-job-definition-2025-08-26-08-34-34-473', 'MonitoringType': 'DataQuality'}]\n",
      "\n",
      "\n",
      "Status: Failed, CreationTime: 2025-08-27 03:08:20.489000+00:00\n",
      "EndpointName: Team2-predictor-endpoint\n",
      "MonitoringType: DataQuality\n",
      "FailureReason: AlgorithmError: Error: Errors occurred when analyzing your data. Please check CloudWatch logs for more details., exit code: 255\n"
     ]
    }
   ],
   "source": [
    "response = sagemaker_client.list_monitoring_executions(\n",
    "    MonitoringScheduleName=schedule_name,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=5\n",
    ")\n",
    "\n",
    "print(response['MonitoringExecutionSummaries'])\n",
    "print(\"\\n\")\n",
    "for job in response['MonitoringExecutionSummaries']:\n",
    "    print(f\"Status: {job['MonitoringExecutionStatus']}, CreationTime: {job['CreationTime']}\")\n",
    "    print(f\"EndpointName: {job['EndpointName']}\")\n",
    "    print(f\"MonitoringType: {job['MonitoringType']}\")\n",
    "    print(f\"FailureReason: {job['FailureReason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a1622-2867-4116-a310-5c638f3a2e3d",
   "metadata": {},
   "source": [
    "-----\n",
    "#### Manual check baseline and capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1efe5ecf-0b12-421d-a61f-d483c94abef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T06:00:39.409523Z",
     "iopub.status.busy": "2025-08-27T06:00:39.408610Z",
     "iopub.status.idle": "2025-08-27T06:00:39.479838Z",
     "shell.execute_reply": "2025-08-27T06:00:39.479070Z",
     "shell.execute_reply.started": "2025-08-27T06:00:39.409484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 .jsonl files for captured data.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "\n",
    "bucket_name = \"iti113-team2-bucket\"\n",
    "capture_prefix = \"Team2/monitoring/data-capture/\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects under data-capture folder\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=capture_prefix)\n",
    "\n",
    "# Filter for .jsonl files (if any)\n",
    "jsonl_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.jsonl')]\n",
    "\n",
    "print(f\"Found {len(jsonl_files)} .jsonl files for captured data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2be5f1-2cee-40eb-9efb-dbf221c45caf",
   "metadata": {},
   "source": [
    "#### load capture jsonl, capture stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f9d09d48-ca9c-4e27-88e4-d7bbef1dbdb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:32:42.369660Z",
     "iopub.status.busy": "2025-08-27T05:32:42.369365Z",
     "iopub.status.idle": "2025-08-27T05:32:42.566416Z",
     "shell.execute_reply": "2025-08-27T05:32:42.565713Z",
     "shell.execute_reply.started": "2025-08-27T05:32:42.369640Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading last file: Team2/monitoring/data-capture/Team2-predictor-endpoint/AllTraffic/2025/08/27/05/16-26-766-20ea9f40-7337-4d43-b630-333a3ae086c7.jsonl\n",
      "Captured data from last file loaded with shape: (100, 25)\n",
      "   restingBP  serumcholestrol  maxheartrate   oldpeak  gender_0  gender_1  \\\n",
      "0          1                1      0.980974  0.228096       0.0       1.0   \n",
      "1          1                1      1.566229 -1.574342       0.0       1.0   \n",
      "2          1                1      1.185813 -1.516199       1.0       0.0   \n",
      "3          1                1      0.659084  1.681675       0.0       1.0   \n",
      "4          1                1      1.185813 -0.527765       1.0       0.0   \n",
      "\n",
      "   chestpain_0  chestpain_1  chestpain_2  chestpain_3  ...  exerciseangia_0  \\\n",
      "0          1.0          0.0          0.0          0.0  ...              1.0   \n",
      "1          1.0          0.0          0.0          0.0  ...              0.0   \n",
      "2          0.0          1.0          0.0          0.0  ...              1.0   \n",
      "3          1.0          0.0          0.0          0.0  ...              0.0   \n",
      "4          0.0          0.0          1.0          0.0  ...              0.0   \n",
      "\n",
      "   exerciseangia_1  slope_0  slope_1  slope_2  slope_3  noofmajorvessels_0  \\\n",
      "0              0.0      0.0      0.0      1.0      0.0                 0.0   \n",
      "1              1.0      1.0      0.0      0.0      0.0                 1.0   \n",
      "2              0.0      0.0      1.0      0.0      0.0                 0.0   \n",
      "3              1.0      0.0      1.0      0.0      0.0                 0.0   \n",
      "4              1.0      0.0      0.0      0.0      1.0                 0.0   \n",
      "\n",
      "   noofmajorvessels_1  noofmajorvessels_2  noofmajorvessels_3  \n",
      "0                 1.0                 0.0                 0.0  \n",
      "1                 0.0                 0.0                 0.0  \n",
      "2                 0.0                 1.0                 0.0  \n",
      "3                 1.0                 0.0                 0.0  \n",
      "4                 1.0                 0.0                 0.0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "                         Mean   Std Dev       Min       Max  Completeness\n",
      "restingBP            1.000000  0.000000  1.000000  1.000000           1.0\n",
      "serumcholestrol      1.000000  0.000000  1.000000  1.000000           1.0\n",
      "maxheartrate        -0.012203  1.084894 -2.179400  1.566229           1.0\n",
      "oldpeak             -0.028897  1.006261 -1.574342  2.030534           1.0\n",
      "gender_0             0.230000  0.422953  0.000000  1.000000           1.0\n",
      "gender_1             0.770000  0.422953  0.000000  1.000000           1.0\n",
      "chestpain_0          0.470000  0.501614  0.000000  1.000000           1.0\n",
      "chestpain_1          0.230000  0.422953  0.000000  1.000000           1.0\n",
      "chestpain_2          0.280000  0.451261  0.000000  1.000000           1.0\n",
      "chestpain_3          0.020000  0.140705  0.000000  1.000000           1.0\n",
      "fastingbloodsugar_0  0.780000  0.416333  0.000000  1.000000           1.0\n",
      "fastingbloodsugar_1  0.220000  0.416333  0.000000  1.000000           1.0\n",
      "restingrelectro_0    0.450000  0.500000  0.000000  1.000000           1.0\n",
      "restingrelectro_1    0.320000  0.468826  0.000000  1.000000           1.0\n",
      "restingrelectro_2    0.230000  0.422953  0.000000  1.000000           1.0\n",
      "exerciseangia_0      0.460000  0.500908  0.000000  1.000000           1.0\n",
      "exerciseangia_1      0.540000  0.500908  0.000000  1.000000           1.0\n",
      "slope_0              0.160000  0.368453  0.000000  1.000000           1.0\n",
      "slope_1              0.320000  0.468826  0.000000  1.000000           1.0\n",
      "slope_2              0.350000  0.479372  0.000000  1.000000           1.0\n",
      "slope_3              0.170000  0.377525  0.000000  1.000000           1.0\n",
      "noofmajorvessels_0   0.270000  0.446196  0.000000  1.000000           1.0\n",
      "noofmajorvessels_1   0.370000  0.485237  0.000000  1.000000           1.0\n",
      "noofmajorvessels_2   0.270000  0.446196  0.000000  1.000000           1.0\n",
      "noofmajorvessels_3   0.090000  0.287623  0.000000  1.000000           1.0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "bucket_name = \"iti113-team2-bucket\"\n",
    "capture_prefix = \"Team2/monitoring/data-capture/\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=capture_prefix)\n",
    "jsonl_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.jsonl')]\n",
    "\n",
    "if not jsonl_files:\n",
    "    print(\"No JSONL files found in the capture folder.\")\n",
    "else:\n",
    "    # Optionally sort keys by LastModified if you want most recent\n",
    "    # Here assuming keys sorted alphabetically is enough\n",
    "    last_file_key = jsonl_files[-1]\n",
    "    print(f\"Loading last file: {last_file_key}\")\n",
    "\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=last_file_key)\n",
    "    content = obj['Body'].read().decode('utf-8')\n",
    "\n",
    "    all_records = []\n",
    "    for line in content.strip().split(\"\\n\"):\n",
    "        event = json.loads(line)\n",
    "        input_data_str = event.get(\"captureData\", {}).get(\"endpointInput\", {}).get(\"data\")\n",
    "        if input_data_str:\n",
    "            input_data = json.loads(input_data_str)\n",
    "            records = input_data.get(\"data\", [])\n",
    "            all_records.extend(records)\n",
    "\n",
    "    df_captured = pd.DataFrame(all_records)\n",
    "    df_captured = df_captured.apply(pd.to_numeric, errors='coerce')\n",
    "    print(f\"Captured data from last file loaded with shape: {df_captured.shape}\")\n",
    "    print(df_captured.head())\n",
    "\n",
    "def calc_stats(df):\n",
    "    stats = {}\n",
    "    for col in df.columns:\n",
    "        col_data = df[col]\n",
    "        stats[col] = {\n",
    "            \"Mean\": col_data.mean(),\n",
    "            \"Std Dev\": col_data.std(),\n",
    "            \"Min\": col_data.min(),\n",
    "            \"Max\": col_data.max(),\n",
    "            \"Completeness\": col_data.notna().mean()\n",
    "        }\n",
    "    return pd.DataFrame(stats).T\n",
    "    \n",
    "captured_stats = calc_stats(df_captured)\n",
    "print(captured_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21454d89-04f0-4636-a361-9c2793a3eca7",
   "metadata": {},
   "source": [
    "#### load baseline stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4e913600-337d-485a-be62-3b34226ea64f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:32:56.323560Z",
     "iopub.status.busy": "2025-08-27T05:32:56.323283Z",
     "iopub.status.idle": "2025-08-27T05:32:56.465921Z",
     "shell.execute_reply": "2025-08-27T05:32:56.465241Z",
     "shell.execute_reply.started": "2025-08-27T05:32:56.323538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Mean  Std Dev      Min      Max  Completeness\n",
      "Feature                                                              \n",
      "restingBP            0.01629  1.00516 -1.92810  1.61111           1.0\n",
      "serumcholestrol      0.01900  0.99702 -2.35272  2.19488           1.0\n",
      "maxheartrate         0.02130  0.98055 -2.17940  1.65402           1.0\n",
      "oldpeak              0.00054  1.00526 -1.57434  2.03053           1.0\n",
      "gender_0             0.23500  0.42400  0.00000  1.00000           1.0\n",
      "gender_1             0.76500  0.42400  0.00000  1.00000           1.0\n",
      "chestpain_0          0.41625  0.49294  0.00000  1.00000           1.0\n",
      "chestpain_1          0.22500  0.41758  0.00000  1.00000           1.0\n",
      "chestpain_2          0.31125  0.46300  0.00000  1.00000           1.0\n",
      "chestpain_3          0.04750  0.21271  0.00000  1.00000           1.0\n",
      "fastingbloodsugar_0  0.69750  0.45934  0.00000  1.00000           1.0\n",
      "fastingbloodsugar_1  0.30250  0.45934  0.00000  1.00000           1.0\n",
      "restingrelectro_0    0.47125  0.49917  0.00000  1.00000           1.0\n",
      "restingrelectro_1    0.34000  0.47371  0.00000  1.00000           1.0\n",
      "restingrelectro_2    0.18875  0.39131  0.00000  1.00000           1.0\n",
      "exerciseangia_0      0.50750  0.49994  0.00000  1.00000           1.0\n",
      "exerciseangia_1      0.49250  0.49994  0.00000  1.00000           1.0\n",
      "slope_0              0.18750  0.39031  0.00000  1.00000           1.0\n",
      "slope_1              0.29375  0.45548  0.00000  1.00000           1.0\n",
      "slope_2              0.31375  0.46402  0.00000  1.00000           1.0\n",
      "slope_3              0.20500  0.40370  0.00000  1.00000           1.0\n",
      "noofmajorvessels_0   0.27875  0.44838  0.00000  1.00000           1.0\n",
      "noofmajorvessels_1   0.34875  0.47657  0.00000  1.00000           1.0\n",
      "noofmajorvessels_2   0.25750  0.43726  0.00000  1.00000           1.0\n",
      "noofmajorvessels_3   0.11500  0.31902  0.00000  1.00000           1.0\n",
      "target               0.57875  0.49376  0.00000  1.00000           1.0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bucket_name = \"iti113-team2-bucket\"\n",
    "baseline_key = \"Team2/monitoring/baseline/statistics.json\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Load JSON from S3\n",
    "response = s3.get_object(Bucket=bucket_name, Key=baseline_key)\n",
    "baseline_stats = json.loads(response['Body'].read().decode('utf-8'))\n",
    "\n",
    "# Extract numerical stats into a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for feature in baseline_stats[\"features\"]:\n",
    "    name = feature[\"name\"]\n",
    "    if \"numerical_statistics\" in feature:\n",
    "        stats = feature[\"numerical_statistics\"]\n",
    "        rows.append({\n",
    "            \"Feature\": name,\n",
    "            \"Mean\": round(stats.get(\"mean\", None), 5),\n",
    "            \"Std Dev\": round(stats.get(\"std_dev\", None), 5),\n",
    "            \"Min\": round(stats.get(\"min\", None), 5),\n",
    "            \"Max\": round(stats.get(\"max\", None), 5),\n",
    "            \"Completeness\": stats.get(\"completeness\", None)\n",
    "            # \"Present\": stats.get(\"common\", {}).get(\"num_present\"),\n",
    "            # \"Missing\": stats.get(\"common\", {}).get(\"num_missing\"),\n",
    "        })\n",
    "\n",
    "df_baseline = pd.DataFrame(rows)\n",
    "df_baseline = df_baseline.set_index(\"Feature\")\n",
    "\n",
    "print(df_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab615002-8130-444f-93ea-ab38c96f3796",
   "metadata": {},
   "source": [
    "#### compare stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "170d6442-c3e3-4b28-892f-de72120031c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:37:07.109947Z",
     "iopub.status.busy": "2025-08-27T05:37:07.109666Z",
     "iopub.status.idle": "2025-08-27T05:37:07.119577Z",
     "shell.execute_reply": "2025-08-27T05:37:07.118829Z",
     "shell.execute_reply.started": "2025-08-27T05:37:07.109927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "def compare_stats(baseline_df, captured_df, mean_threshold=1.5, completeness_threshold=0.05, tolerance=1e-6, s3_bucket=None, s3_key=None):\n",
    "    violations = []\n",
    "\n",
    "    # Align feature names\n",
    "    common_features = baseline_df.index.intersection(captured_df.index)\n",
    "    baseline_df = baseline_df.loc[common_features]\n",
    "    captured_df = captured_df.loc[common_features]\n",
    "    \n",
    "    for feature in common_features:\n",
    "        base = baseline_df.loc[feature]\n",
    "        cap = captured_df.loc[feature]\n",
    "\n",
    "        # Mean drift check\n",
    "        mean_diff = abs(cap[\"Mean\"] - base[\"Mean\"])\n",
    "        std = base[\"Std Dev\"]\n",
    "        if std > 0:\n",
    "            mean_limit = mean_threshold * std\n",
    "            if mean_diff > mean_limit + tolerance:\n",
    "                violations.append({\n",
    "                    \"feature\": feature,\n",
    "                    \"violation\": f\"Mean drift too high: Baseline mean={base['Mean']:.4f}, Captured mean={cap['Mean']:.4f}\"\n",
    "                })\n",
    "\n",
    "        # Completeness drop\n",
    "        completeness_diff = base[\"Completeness\"] - cap[\"Completeness\"]\n",
    "        if completeness_diff > completeness_threshold:\n",
    "            violations.append({\n",
    "                \"feature\": feature,\n",
    "                \"violation\": f\"Completeness dropped by {completeness_diff:.2%}\"\n",
    "            })\n",
    "\n",
    "        # Min below baseline min\n",
    "        if not np.isclose(cap[\"Min\"], base[\"Min\"], atol=tolerance) and cap[\"Min\"] < base[\"Min\"]:\n",
    "            violations.append({\n",
    "                \"feature\": feature,\n",
    "                \"violation\": f\"Captured min {cap['Min']:.4f} below baseline min {base['Min']:.4f}\"\n",
    "            })\n",
    "\n",
    "        # Max above baseline max\n",
    "        if not np.isclose(cap[\"Max\"], base[\"Max\"], atol=tolerance) and cap[\"Max\"] > base[\"Max\"]:\n",
    "            violations.append({\n",
    "                \"feature\": feature,\n",
    "                \"violation\": f\"Captured max {cap['Max']:.4f} above baseline max {base['Max']:.4f}\"\n",
    "            })\n",
    "\n",
    "        # Variance collapse\n",
    "        if cap[\"Std Dev\"] < tolerance and base[\"Std Dev\"] > 0:\n",
    "            violations.append({\n",
    "                \"feature\": feature,\n",
    "                \"violation\": f\"Variance dropped to zero in capture: Baseline std={base['Std Dev']:.4f}, Captured std={cap['Std Dev']:.4f}\"\n",
    "            })\n",
    "\n",
    "        if base[\"Std Dev\"] < tolerance and cap[\"Std Dev\"] < tolerance:\n",
    "            violations.append({\n",
    "                \"feature\": feature,\n",
    "                \"violation\": \"Both baseline and capture have near-zero variance (constant feature)\"\n",
    "            })\n",
    "\n",
    "    # Convert violations to DataFrame\n",
    "    violations_df = pd.DataFrame(violations)\n",
    "\n",
    "    if s3_bucket and s3_key:\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        json_str = violations_df.to_json(orient=\"records\", lines=False)  # JSON array string\n",
    "        s3.put_object(Bucket=s3_bucket, Key=s3_key, Body=json_str)\n",
    "        print(f\"Violation report saved to s3://{s3_bucket}/{s3_key}\")\n",
    "        \n",
    "        # csv_buffer = StringIO()\n",
    "        # violations_df.to_csv(csv_buffer, index=False)\n",
    "        # s3.put_object(Bucket=s3_bucket, Key=s3_key, Body=csv_buffer.getvalue())\n",
    "        # print(f\"Violation report saved to s3://{s3_bucket}/{s3_key}\")\n",
    "\n",
    "    return violations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f91672fd-e343-4b22-a991-e7de7e0c13f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:37:17.532893Z",
     "iopub.status.busy": "2025-08-27T05:37:17.532169Z",
     "iopub.status.idle": "2025-08-27T05:37:17.609858Z",
     "shell.execute_reply": "2025-08-27T05:37:17.609053Z",
     "shell.execute_reply.started": "2025-08-27T05:37:17.532867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation report saved to s3://iti113-team2-bucket/Team2/monitoring/reports/violation_report.json\n",
      "           feature                                          violation\n",
      "0        restingBP  Variance dropped to zero in capture: Baseline ...\n",
      "1  serumcholestrol  Variance dropped to zero in capture: Baseline ...\n"
     ]
    }
   ],
   "source": [
    "# violation_report = compare_stats(df_baseline, captured_stats)\n",
    "report_df = compare_stats(\n",
    "    df_baseline,\n",
    "    captured_stats,\n",
    "    mean_threshold=1.5,\n",
    "    completeness_threshold=0.05,\n",
    "    s3_bucket=bucket_name,\n",
    "    s3_key=\"Team2/monitoring/reports/violation_report.json\"\n",
    ")\n",
    "print(report_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8dae90-5298-478b-ae2b-b49c06307004",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "-----\n",
    "#### Check if DataCaptureConfig enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cce1c62-9a3e-4aef-b432-fef2ef4ad1c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T02:56:33.881628Z",
     "iopub.status.busy": "2025-08-27T02:56:33.881326Z",
     "iopub.status.idle": "2025-08-27T02:56:34.073305Z",
     "shell.execute_reply": "2025-08-27T02:56:34.072576Z",
     "shell.execute_reply.started": "2025-08-27T02:56:33.881605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data capture is enabled.\n",
      "{'EnableCapture': True, 'CaptureStatus': 'Started', 'CurrentSamplingPercentage': 100, 'DestinationS3Uri': 's3://iti113-team2-bucket/Team2/monitoring/data-capture'}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "endpoint_name = \"Team2-predictor-endpoint\"\n",
    "\n",
    "# Get endpoint config name\n",
    "endpoint_response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_config_name = endpoint_response[\"EndpointConfigName\"]\n",
    "\n",
    "config_response = sagemaker_client.describe_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "capture_config = config_response.get(\"DataCaptureConfig\")\n",
    "\n",
    "if capture_config and capture_config.get(\"EnableCapture\"):\n",
    "    print(\"✅ Data capture is enabled.\")\n",
    "    print(endpoint_response['DataCaptureConfig'])\n",
    "\n",
    "else:\n",
    "    print(\"❌ Data capture is NOT enabled. Monitoring cannot be scheduled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8246c5-a25e-4699-8e39-4759c8d72a9d",
   "metadata": {},
   "source": [
    "#### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "401f102a-62fa-4a1c-939b-967be8696060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T05:59:49.896930Z",
     "iopub.status.busy": "2025-08-27T05:59:49.896268Z",
     "iopub.status.idle": "2025-08-27T06:00:00.521405Z",
     "shell.execute_reply": "2025-08-27T06:00:00.504894Z",
     "shell.execute_reply.started": "2025-08-27T05:59:49.896896Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting Monitoring Schedule with name: Team2-drift-schedule-main\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Deleting Data Quality Job Definition with name: data-quality-job-definition-2025-08-27-05-20-52-027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring schedule 'Team2-drift-schedule-main' deleted.\n"
     ]
    }
   ],
   "source": [
    "# Clean up resources\n",
    "try:\n",
    "    # Delete the Monitoring Schedule\n",
    "    monitor.delete_monitoring_schedule()\n",
    "    print(f\"Monitoring schedule '{schedule_name}' deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not delete schedule: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228b111-60e0-46ea-8b80-ea9b31eaa525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
